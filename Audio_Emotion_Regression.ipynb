{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chesseraf/Audio-emotion-recognition/blob/main/Audio_Emotion_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RTmiRZfIcVwO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "DRIVE_MOUNTED = False\n",
        "if DRIVE_MOUNTED:\n",
        "  drive.mount('/content/drive')\n",
        "  SAVE_DIR = \"/content/drive/MyDrive/w2v2_temporal_head\"\n",
        "  DATA_DIR     = \"/content/drive/MyDrive/Colab_Drive_Files\"   # folder with .wav or .mp3 + matching .json\n",
        "else:\n",
        "  SAVE_DIR = \"./w2v2_temporal_head\"\n",
        "  DATA_DIR = \"./Data\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjw-m6TYMcT6",
        "outputId": "32f5c730-c244-4fed-996a-602f675b1600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1.6/2.4 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.3/2.4 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install torchcodec --index-url \"https://download.pytorch.org/whl/cu126\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gswZvVP-vnLv"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os, gc, torch, json, random, math, torchaudio, joblib\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoFeatureExtractor, AutoConfig, Wav2Vec2Model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rRfPgqacwLB",
        "outputId": "19486ec1-02e4-423a-f4aa-77ca6838a1b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.9.0+cu126 | CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"USE_TF\"] = \"0\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P1Q_KHsTc5DQ"
      },
      "outputs": [],
      "source": [
        "# ==== USER SETTINGS ====\n",
        "\n",
        "TARGET_KEYS  = [\"Valence_best\",\"Arousal_best\",\"Submissive_vs._Dominant_best\", \"Serious_vs._Humorous_best\"]\n",
        "MODEL_NAME   = \"facebook/wav2vec2-base-960h\"  # small & stable; upgrade later if needed\n",
        "TARGET_SR    = 16_000\n",
        "MAX_SECONDS  = 12.0              # keep modest; you can try more later\n",
        "SEED         = 42\n",
        "\n",
        "# Training\n",
        "EPOCHS       = 50                # start small\n",
        "LR           = 1e-3             # higher LR since we train only a tiny head\n",
        "WEIGHT_DECAY = 0.007             # overfits with current data set size\n",
        "BATCH_SIZE   = 1                # keep at 1 for stability\n",
        "NUM_WORKERS  = 0                # 0 = no multiprocessing (stable on Colab)\n",
        "VAL_SPLIT    = 0.1              # if N>1, take ~10% for val\n",
        "MAX_FILES    = None             # set an int (e.g., 200) for a smoke test; None = all\n",
        "\n",
        "FEATURE_FILE_EXTENSION = '.npy' # encoder outputs saved as numpy tensors\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZyqMiT2UDXD3"
      },
      "outputs": [],
      "source": [
        "# Normalization based on label ranges of different attributes\n",
        "ALL_KEYS_WITH_RANGE = {\n",
        "  \"Valence_best\": { \"min\": -3, \"max\": 3 },\n",
        "  \"Arousal_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Submissive_vs._Dominant_best\": { \"min\": -3, \"max\": 3 },\n",
        "  \"Age_best\": { \"min\": 0, \"max\": 6 },\n",
        "  \"Gender_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Serious_vs._Humorous_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Vulnerable_vs._Emotionally_Detached_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Confident_vs._Hesitant_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Warm_vs._Cold_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Monotone_vs._Expressive_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"High-Pitched_vs._Low-Pitched_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Soft_vs._Harsh_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Authenticity_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Recording_Quality_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Background_Noise_best\": { \"min\": 0, \"max\": 3 }\n",
        "}\n",
        "\n",
        "def normalize_range(value, attribute):\n",
        "    min_value = ALL_KEYS_WITH_RANGE[attribute][\"min\"]\n",
        "    max_value = ALL_KEYS_WITH_RANGE[attribute][\"max\"]\n",
        "    return (value - min_value) / (max_value - min_value)\n",
        "def denormalize_range(value, attribute):\n",
        "    min_value = ALL_KEYS_WITH_RANGE[attribute][\"min\"]\n",
        "    max_value = ALL_KEYS_WITH_RANGE[attribute][\"max\"]\n",
        "    return value * (max_value - min_value) + min_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "975c6090",
        "outputId": "f84f9245-169d-485c-b01e-4f65d6599d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LazyAudioData class defined.\n"
          ]
        }
      ],
      "source": [
        "global num_mp3s_encoded\n",
        "global reused_encodings\n",
        "global presaved_encoding_found\n",
        "num_mp3s_encoded = 0\n",
        "reused_encodings = 0\n",
        "presaved_encoding_found = 0\n",
        "\n",
        "# Audio and emotions will be lazy initialized, and also store their output of the base encoder\n",
        "\n",
        "# Assuming normalize_range, load_first_n_seconds, TARGET_KEYS, TARGET_SR, MAX_SECONDS, fe, device are defined in the global scope\n",
        "\n",
        "class LazyAudioData:\n",
        "    def __init__(self, audio_path: str, json_path: str, existing_feature_path: str):\n",
        "        self._audio_path = audio_path\n",
        "        self._json_path = json_path\n",
        "        self._precalculated_feature_file = existing_feature_path\n",
        "        self._emotions = None  # To store cached emotions\n",
        "        self._wav = None       # To store cached waveform\n",
        "        self._encoded_features = None # To store cached encoded features\n",
        "\n",
        "    @property\n",
        "    def emotions(self) -> List[float]:\n",
        "\n",
        "        if self._emotions is None:\n",
        "            # Load and normalize emotions from JSON using global TARGET_KEYS and normalize_range\n",
        "            try:\n",
        "                emo = json.loads(Path(self._json_path).read_text(encoding=\"utf-8\")).get(\"emotion_annotation\", {})\n",
        "                labels = [normalize_range(float(emo[k]), k) for k in TARGET_KEYS]\n",
        "                if not all(np.isfinite(labels)):\n",
        "                    raise ValueError(\"Non-finite labels found after normalization\")\n",
        "                if isinstance(labels, torch.Tensor):\n",
        "                    labels = labels.unsqueeze(0)          # (1, D)\n",
        "                else:\n",
        "                    labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
        "                labels = labels.to(device)\n",
        "                self._emotions = labels\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading emotions from {self._json_path}: {e}\")\n",
        "                self._emotions = [] # Return empty list on error\n",
        "        return self._emotions\n",
        "\n",
        "    @property\n",
        "    def wav(self) -> torch.Tensor:\n",
        "        if self._wav is None:\n",
        "            # Load waveform using the existing global function and parameters\n",
        "            try:\n",
        "                self._wav = load_first_n_seconds(self._audio_path, TARGET_SR, MAX_SECONDS)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading audio from {self._audio_path}: {e}\")\n",
        "                self._wav = torch.empty(0) # Return empty tensor on error\n",
        "        return self._wav\n",
        "\n",
        "    @property\n",
        "    def encoded_features(self):\n",
        "        global num_mp3s_encoded\n",
        "        global reused_encodings\n",
        "        global presaved_encoding_found\n",
        "        if self._encoded_features is None:\n",
        "            # check if it has been computed and saved before\n",
        "            precalculated_feature = None\n",
        "\n",
        "            # was slow as implemented\n",
        "            if self._precalculated_feature_file is not None:\n",
        "                try:\n",
        "                  arr = np.load(Path(self._precalculated_feature_file))\n",
        "                  self._encoded_features = torch.from_numpy(arr).cuda()             # CPU tensor\n",
        "                  presaved_encoding_found += 1\n",
        "                  if presaved_encoding_found % 100 == 0:\n",
        "                    print(f\"Found {presaved_encoding_found} presaved encodings\")\n",
        "                  return self._encoded_features\n",
        "                except Exception as e:\n",
        "                  print(f\"Warning: Error loading precalculated features from {self._precalculated_feature_file}: {e}\")\n",
        "\n",
        "            # Ensure wav is loaded first\n",
        "            if self._wav is None:\n",
        "                _ = self.wav # Trigger wav loading\n",
        "            if self._wav is not None and self._wav.numel() > 0:\n",
        "                # Process wav through feature extractor (global `fe`)\n",
        "                # Feat should be moved to device for consistency with batch_to_inputs\n",
        "                feat = fe(self._wav, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=\"do_not_pad\")\n",
        "\n",
        "                # THESE ARE ENCODER INPUTS, NOT FEATURES\n",
        "                inputs = {k: v.to(device) for k, v in feat.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    out = encoder(input_values=inputs[\"input_values\"])\n",
        "                    self._encoded_features = out.last_hidden_state  # (B, T', d_model)\n",
        "\n",
        "                    # Save the encoded features to a file for future runs\n",
        "                    feature_path = Path(self._audio_path).with_suffix(FEATURE_FILE_EXTENSION)\n",
        "                    np.save(feature_path, self._encoded_features.cpu().numpy())\n",
        "                # dont keep the large wavs in ram\n",
        "                del self._wav\n",
        "                self._wav = None\n",
        "                num_mp3s_encoded += 1\n",
        "                if num_mp3s_encoded % 100 == 0:\n",
        "                    print(f\"Processed {num_mp3s_encoded} mp3s\")\n",
        "        else:\n",
        "          reused_encodings += 1\n",
        "          if reused_encodings == 1:\n",
        "            print(f\"Reused an encoding!\")\n",
        "\n",
        "        return self._encoded_features\n",
        "\n",
        "print(\"LazyAudioData class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4YjYyNSc-HN",
        "outputId": "c5c00e22-d1ed-4ce2-c684-41a184187ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pairs total=23  train=21  val=2\n"
          ]
        }
      ],
      "source": [
        "def collect_pairs(data_dir: str, target_keys: List[str], limit: Optional[int]=None):\n",
        "    root = Path(data_dir)\n",
        "    files = sorted(root.rglob(\"*.mp3\"))\n",
        "    items = []\n",
        "    for mp3 in files:\n",
        "        j = mp3.with_suffix(\".json\")\n",
        "        if not j.exists():\n",
        "            continue\n",
        "        precalculated_feature_file = mp3.with_suffix(FEATURE_FILE_EXTENSION)\n",
        "        if not precalculated_feature_file.exists():\n",
        "            calculatedFile = None\n",
        "        else:\n",
        "            calculatedFile = str(precalculated_feature_file)\n",
        "\n",
        "        items.append(LazyAudioData(str(mp3), str(j), calculatedFile))\n",
        "    if not items:\n",
        "        raise RuntimeError(\"No usable (audio,json) pairs found.\")\n",
        "    return items\n",
        "\n",
        "items = collect_pairs(DATA_DIR, TARGET_KEYS, limit=MAX_FILES)\n",
        "random.shuffle(items)\n",
        "\n",
        "# Split\n",
        "n = len(items)\n",
        "if n == 1:\n",
        "    train_items, val_items = items, []\n",
        "else:\n",
        "    n_val = min(max(1, int(n * VAL_SPLIT)), n-1)\n",
        "    val_items, train_items = items[:n_val], items[n_val:]\n",
        "\n",
        "print(f\"pairs total={n}  train={len(train_items)}  val={len(val_items)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DXNs3B1_dBmp"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = int(TARGET_SR * MAX_SECONDS)\n",
        "_resamplers: Dict[Tuple[int,int], torchaudio.transforms.Resample] = {}\n",
        "\n",
        "def load_first_n_seconds(path: str, target_sr: int, max_seconds: float) -> torch.Tensor:\n",
        "    # infer original SR without decoding full file\n",
        "    try:\n",
        "        info = torchaudio.info(path)\n",
        "        orig_sr = info.sample_rate\n",
        "    except Exception:\n",
        "        _, orig_sr = torchaudio.load(path, frame_offset=0, num_frames=1024)\n",
        "    frames = int(orig_sr * max_seconds)\n",
        "\n",
        "    # read only that window\n",
        "    wav, sr = torchaudio.load(path, frame_offset=0, num_frames=frames)  # (C, T<=frames)\n",
        "\n",
        "    # mono\n",
        "    if wav.shape[0] > 1:\n",
        "        wav = wav.mean(0, keepdim=True)\n",
        "    # resample minimal window\n",
        "    if sr != target_sr:\n",
        "        key = (sr, target_sr)\n",
        "        if key not in _resamplers:\n",
        "            _resamplers[key] = torchaudio.transforms.Resample(sr, target_sr)\n",
        "        wav = _resamplers[key](wav)\n",
        "    wav = wav.squeeze(0)\n",
        "\n",
        "    # truncate/pad to EXACT MAX_LEN (so we can use padding=\"do_not_pad\")\n",
        "    if wav.numel() > MAX_LEN:\n",
        "        wav = wav[:MAX_LEN]\n",
        "    if wav.numel() < MAX_LEN:\n",
        "        wav = torch.nn.functional.pad(wav, (0, MAX_LEN - wav.numel()))\n",
        "\n",
        "    # peak normalize\n",
        "    wav = wav / (wav.abs().max() + 1e-9)\n",
        "    return wav\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEMDizHKdLx1",
        "outputId": "16bef962-263d-4720-f697-f7d7f994ee49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders ready.\n"
          ]
        }
      ],
      "source": [
        "class PathDataset(Dataset):\n",
        "    def __init__(self, items: List[Dict[str,Any]]):\n",
        "        self.items = items\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.items[idx]\n",
        "        return {\"encodedFeatures\": ex.encoded_features, \"labels\": ex.emotions}\n",
        "\n",
        "train_ds = PathDataset(train_items)\n",
        "val_ds   = PathDataset(val_items) if len(val_items) else None\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
        "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False) if val_ds else None\n",
        "\n",
        "print(\"Loaders ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWGws16-dM0O",
        "outputId": "7448f3fd-d292-4990-eee8-881fd3996961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder frozen. Trainable head params: 724229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2202145022.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n"
          ]
        }
      ],
      "source": [
        "# Feature extractor & encoder (frozen)\n",
        "fe = AutoFeatureExtractor.from_pretrained(MODEL_NAME, sampling_rate=TARGET_SR)\n",
        "enc_cfg = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=False)\n",
        "encoder = Wav2Vec2Model.from_pretrained(MODEL_NAME, config=enc_cfg).to(device)\n",
        "encoder.eval()\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Tiny temporal head: GRU + attention pooling -> 3 outputs\n",
        "class TemporalHead(nn.Module):\n",
        "    def __init__(self, d_model=768, hidden=128, out_dim=len(TARGET_KEYS)):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(d_model, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Linear(2*hidden, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.LayerNorm(2*hidden),\n",
        "            nn.Linear(2*hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, hs):                 # hs: (B, T', d_model)\n",
        "        z, _ = self.gru(hs)                # (B, T', 2H)\n",
        "        a = self.att(z).squeeze(-1)        # (B, T')\n",
        "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
        "        pooled = (w * z).sum(dim=1)        # (B, 2H)\n",
        "        return self.out(pooled)            # (B, out_dim)\n",
        "\n",
        "num_labels = len(TARGET_KEYS)\n",
        "head = TemporalHead(d_model=encoder.config.hidden_size, hidden=128, out_dim=num_labels).to(device)\n",
        "\n",
        "# Only head is trainable\n",
        "opt = torch.optim.AdamW(head.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "mse = nn.MSELoss()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
        "\n",
        "print(\"Encoder frozen. Trainable head params:\",\n",
        "      sum(p.numel() for p in head.parameters() if p.requires_grad))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WtM5rEJgYJKT"
      },
      "outputs": [],
      "source": [
        "# run partial validation report every so often\n",
        "global training_count_report # every so many training samples make and print a report\n",
        "training_count_report = 410\n",
        "global val_count_to_report\n",
        "val_count_to_report = 400\n",
        "global report_num\n",
        "report_num = 0\n",
        "def run_val_report(numsamples=val_count_to_report, mini_train_loss=None):\n",
        "  global report_num\n",
        "  report_num += 1\n",
        "  with torch.no_grad():\n",
        "    va_loss = run_epoch(val_loader, train=False, limit_samples=numsamples)\n",
        "    print(f\"report {report_num} | val MSE={va_loss:.4f}\")\n",
        "    if mini_train_loss is not None:\n",
        "      print(f\"report {report_num} | train MSE={mini_train_loss/training_count_report:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4eXYz08dR3g",
        "outputId": "7fb3854c-b8b6-4657-f23d-952111b377c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:634: UserWarning: Using a target size (torch.Size([1, 1, 4])) that is different to the input size (torch.Size([1, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000 | val MSE=0.1127\n",
            "Reused an encoding!\n",
            "Epoch 1/1000 | train MSE=0.4230 | val MSE=0.0501\n",
            "Epoch 2/1000 | train MSE=0.0467 | val MSE=0.0044\n",
            "Epoch 3/1000 | train MSE=0.0129 | val MSE=0.0231\n",
            "Epoch 4/1000 | train MSE=0.0141 | val MSE=0.0048\n",
            "Epoch 5/1000 | train MSE=0.0092 | val MSE=0.0442\n",
            "Epoch 6/1000 | train MSE=0.0096 | val MSE=0.0065\n",
            "Epoch 7/1000 | train MSE=0.0046 | val MSE=0.0062\n",
            "Epoch 8/1000 | train MSE=0.0043 | val MSE=0.0048\n",
            "Epoch 9/1000 | train MSE=0.0044 | val MSE=0.0113\n",
            "Epoch 10/1000 | train MSE=0.0035 | val MSE=0.0084\n",
            "Epoch 11/1000 | train MSE=0.0022 | val MSE=0.0052\n",
            "Epoch 12/1000 | train MSE=0.0015 | val MSE=0.0037\n"
          ]
        }
      ],
      "source": [
        "def run_epoch(loader, train=True, limit_samples=None):\n",
        "    if train:\n",
        "        head.train()\n",
        "    else:\n",
        "        head.eval()\n",
        "    total_loss = 0.0\n",
        "    mini_report_loss = 0.0\n",
        "    sample_num = 0\n",
        "    # batch is only one sample\n",
        "    for batch in loader:\n",
        "        sample_num += 1\n",
        "        if limit_samples is not None and sample_num > limit_samples:\n",
        "            break\n",
        "        # intermediate reports can be run when data set is large and slow\n",
        "        # if sample_num % training_count_report == 0:\n",
        "        #   run_val_report(mini_train_loss=mini_report_loss)\n",
        "        #   mini_report_loss = 0\n",
        "        #   if train: # Ensure head is back in train mode after validation report\n",
        "        #     head.train()\n",
        "\n",
        "        hs, labels = batch[\"encodedFeatures\"].squeeze(0), batch[\"labels\"]\n",
        "        with torch.amp.autocast('cuda', enabled=(device.type==\"cuda\")):\n",
        "            preds = head(hs)\n",
        "            # average of the 4 emotion's square errors\n",
        "            loss = mse(preds, labels)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "        cLoss = loss.item() * labels.size(0)\n",
        "        total_loss += cLoss\n",
        "        mini_report_loss += cLoss\n",
        "\n",
        "    return total_loss / max(1, sample_num)\n",
        "with torch.no_grad():\n",
        "     va_loss = run_epoch(val_loader, train=False)\n",
        "     print(f\"Epoch {0}/{EPOCHS} | val MSE={va_loss:.4f}\")\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss = run_epoch(train_loader, train=True)\n",
        "    if val_loader:\n",
        "        with torch.no_grad():\n",
        "            va_loss = run_epoch(val_loader, train=False)\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f} | val MSE={va_loss:.4f}\")\n",
        "        if va_loss < 0.0041:\n",
        "            break\n",
        "    else:\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MsUmON-SwMiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91aedb8d-d6c1-456b-95ce-12f06bf29298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n",
            "210\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(num_mp3s_encoded)\n",
        "print(reused_encodings)\n",
        "print(presaved_encoding_found)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KsSrZpMJdWRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deef99e4-bf0f-46cf-94be-bd76eb89123d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reused an encoding!\n",
            "Keys:             ['Valence_best', 'Arousal_best', 'Submissive_vs._Dominant_best', 'Serious_vs._Humorous_best']\n",
            "Validation mean:  [0.4989522  0.3203125  0.53027344 0.11364746]\n",
            "Predicted mean:   [0.49259228 0.33597976 0.5918721  0.21141511]\n",
            "Validation std:  [0.00186156 0.0625     0.04329427 0.06506348]\n",
            "Predicted std:   [0.0148968  0.0493543  0.03088683 0.00143539]\n",
            "Validation metrics:\n",
            "  MAE_macro: 0.0742\n",
            "  MSE_macro: 0.0090\n",
            "  MAE_Valence_best: 0.0130\n",
            "  MSE_Valence_best: 0.0002\n",
            "  CCC_Valence_best: 0.2086\n",
            "  MAE_Arousal_best: 0.1119\n",
            "  MSE_Arousal_best: 0.0128\n",
            "  CCC_Arousal_best: -0.9365\n",
            "  MAE_Submissive_vs._Dominant_best: 0.0742\n",
            "  MSE_Submissive_vs._Dominant_best: 0.0093\n",
            "  CCC_Submissive_vs._Dominant_best: -0.4038\n",
            "  MAE_Serious_vs._Humorous_best: 0.0978\n",
            "  MSE_Serious_vs._Humorous_best: 0.0136\n",
            "  CCC_Serious_vs._Humorous_best: 0.0135\n",
            "Keys:             ['Valence_best', 'Arousal_best', 'Submissive_vs._Dominant_best', 'Serious_vs._Humorous_best']\n",
            "Validation mean:  [0.5003225  0.30203682 0.5056124  0.17589751]\n",
            "Predicted mean:   [0.50787765 0.2865647  0.56986046 0.22656979]\n",
            "Validation std:  [0.00109689 0.10036702 0.0351764  0.05222961]\n",
            "Predicted std:   [0.04050812 0.04665081 0.02851329 0.04936844]\n",
            "Training metrics:\n",
            "  MAE_macro: 0.0596\n",
            "  MSE_macro: 0.0054\n",
            "  MAE_Valence_best: 0.0309\n",
            "  MSE_Valence_best: 0.0017\n",
            "  CCC_Valence_best: 0.0014\n",
            "  MAE_Arousal_best: 0.0769\n",
            "  MSE_Arousal_best: 0.0085\n",
            "  CCC_Arousal_best: 0.3199\n",
            "  MAE_Submissive_vs._Dominant_best: 0.0738\n",
            "  MSE_Submissive_vs._Dominant_best: 0.0064\n",
            "  CCC_Submissive_vs._Dominant_best: -0.0410\n",
            "  MAE_Serious_vs._Humorous_best: 0.0566\n",
            "  MSE_Serious_vs._Humorous_best: 0.0051\n",
            "  CCC_Serious_vs._Humorous_best: 0.3455\n"
          ]
        }
      ],
      "source": [
        "def ccc(y_true, y_pred):\n",
        "    y = np.asarray(y_true, np.float64)\n",
        "    x = np.asarray(y_pred, np.float64)\n",
        "    vx, vy = x.var(), y.var()\n",
        "    mx, my = x.mean(), y.mean()\n",
        "    cov = ((x - mx) * (y - my)).mean()\n",
        "    denom = vx + vy + (mx - my)**2\n",
        "    return float(2 * cov / denom) if denom > 0 else 0.0\n",
        "\n",
        "def evaluate_full(loader):\n",
        "    y_true, y_pred = [], []\n",
        "    head.eval()\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            hs, labels = batch[\"encodedFeatures\"].squeeze(0), batch[\"labels\"]\n",
        "            preds = head(hs)\n",
        "            y_true.append(labels.detach().cpu().numpy())\n",
        "            y_pred.append(preds.detach().cpu().numpy())\n",
        "            torch.cuda.empty_cache()\n",
        "    y_true = np.array(y_true).squeeze(1)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    true_sq = np.array(y_true).squeeze()\n",
        "    pred_sq = np.array(y_pred).squeeze()\n",
        "\n",
        "    mean_true = true_sq.mean(axis=0)\n",
        "    mean_pred = pred_sq.mean(axis=0)\n",
        "    std_true = true_sq.std(axis=0)\n",
        "    std_pred = pred_sq.std(axis=0)\n",
        "    print(\"Keys:            \", TARGET_KEYS)\n",
        "    print(\"Validation mean: \", mean_true)\n",
        "    print(\"Predicted mean:  \", mean_pred)\n",
        "    print(\"Validation std: \", std_true)\n",
        "    print(\"Predicted std:  \", std_pred)\n",
        "\n",
        "    Y = np.concatenate(y_true, axis=0)\n",
        "    P = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    mae = mean_absolute_error(Y, P, multioutput=\"raw_values\")\n",
        "    mse = mean_squared_error(Y, P, multioutput=\"raw_values\")\n",
        "    metrics = {\n",
        "        \"MAE_macro\": float(mae.mean()),\n",
        "        \"MSE_macro\": float(mse.mean()),\n",
        "    }\n",
        "    for i,k in enumerate(TARGET_KEYS):\n",
        "        metrics[f\"MAE_{k}\"] = float(mae[i])\n",
        "        metrics[f\"MSE_{k}\"] = float(mse[i])\n",
        "        metrics[f\"CCC_{k}\"] = ccc(Y[:,i], P[:,i])\n",
        "    return metrics\n",
        "\n",
        "if val_loader and len(val_ds) > 0:\n",
        "    metrics = evaluate_full(val_loader)\n",
        "    print(\"Validation metrics:\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "else:\n",
        "    print(\"No validation split; skipped metrics.\")\n",
        "\n",
        "if train_loader and len(train_ds) > 0:\n",
        "    metrics = evaluate_full(train_loader)\n",
        "    print(\"Training metrics:\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "else:\n",
        "    print(\"No training split; skipped metrics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Da-sMGLJdbMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96737ee-4d23-4782-86db-fd969ba0087b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to ./w2v2_temporal_head\n"
          ]
        }
      ],
      "source": [
        "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save torch head\n",
        "torch.save(head.state_dict(), f\"{SAVE_DIR}/temporal_head.pt\")\n",
        "# Save config to rebuild pipeline later\n",
        "json.dump({\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"target_sr\": TARGET_SR,\n",
        "    \"max_seconds\": MAX_SECONDS,\n",
        "    \"target_keys\": TARGET_KEYS,\n",
        "    \"head\": {\"d_model\": int(encoder.config.hidden_size), \"hidden\": 128, \"out_dim\": len(TARGET_KEYS)}\n",
        "}, open(f\"{SAVE_DIR}/config.json\",\"w\"))\n",
        "\n",
        "print(\"Saved to\", SAVE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KMMkk3W-_ztR"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1) Load config\n",
        "cfg_path = f\"{SAVE_DIR}/config.json\"\n",
        "state_path = f\"{SAVE_DIR}/temporal_head.pt\"\n",
        "\n",
        "with open(cfg_path, \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "MODEL_NAME  = cfg[\"model_name\"]\n",
        "TARGET_SR   = cfg[\"target_sr\"]\n",
        "MAX_SECONDS = cfg[\"max_seconds\"]\n",
        "TARGET_KEYS = cfg[\"target_keys\"]\n",
        "head_cfg    = cfg[\"head\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6SDEfok8_2Rj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4778043-c73f-4d34-aeae-d3e6cce3d607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model reloaded and ready!\n"
          ]
        }
      ],
      "source": [
        "# Reload frozen base encoder\n",
        "fe = AutoFeatureExtractor.from_pretrained(MODEL_NAME, sampling_rate=TARGET_SR)\n",
        "enc_cfg = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=False)\n",
        "encoder = Wav2Vec2Model.from_pretrained(MODEL_NAME, config=enc_cfg).to(device)\n",
        "encoder.eval()\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Recreate the TemporalHead with same dimensions from config\n",
        "class TemporalHead(nn.Module):\n",
        "    def __init__(self, d_model=768, hidden=128, out_dim=len(TARGET_KEYS)):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(d_model, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Linear(2*hidden, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.LayerNorm(2*hidden),\n",
        "            nn.Linear(2*hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, hs):                 # hs: (B, T', d_model)\n",
        "        z, _ = self.gru(hs)                # (B, T', 2H)\n",
        "        a = self.att(z).squeeze(-1)        # (B, T')\n",
        "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
        "        pooled = (w * z).sum(dim=1)        # (B, 2H)\n",
        "        return self.out(pooled)            # (B, out_dim)\n",
        "\n",
        "\n",
        "head = TemporalHead(\n",
        "    d_model=head_cfg[\"d_model\"],\n",
        "    hidden=head_cfg[\"hidden\"],\n",
        "    out_dim=head_cfg[\"out_dim\"],\n",
        ").to(device)\n",
        "\n",
        "# Load trained weights\n",
        "state = torch.load(state_path, map_location=device)\n",
        "head.load_state_dict(state)\n",
        "head.eval()\n",
        "\n",
        "print(\"Model reloaded and ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "L-WXvzQS_4xj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "faa5de01-caca-490d-f36d-6e96858bc53d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to create AudioDecoder for /content/drive/MyDrive/meLaugh.mp3: Could not open input file: /content/drive/MyDrive/meLaugh.mp3 No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-315879173.py\u001b[0m in \u001b[0;36mload_first_n_seconds\u001b[0;34m(path, target_sr, max_seconds)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0morig_sr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torchaudio' has no attribute 'info'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_torchcodec.py\u001b[0m in \u001b[0;36mload_with_torchcodec\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchcodec/decoders/_audio_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, stream_index, sample_rate, num_channels)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchcodec.decoders.AudioDecoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseek_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"approximate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchcodec/decoders/_decoder_utils.py\u001b[0m in \u001b[0;36mcreate_decoder\u001b[0;34m(source, seek_mode)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseek_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Could not open input file: /content/drive/MyDrive/meLaugh.mp3 No such file or directory",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3616362688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# test some examples, including my own voice:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/meLaugh.mp3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3616362688.py\u001b[0m in \u001b[0;36mpredict_attributes\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# 1) Load & preprocess audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_first_n_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_SR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SECONDS\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (T,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-315879173.py\u001b[0m in \u001b[0;36mload_first_n_seconds\u001b[0;34m(path, target_sr, max_seconds)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0morig_sr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_sr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mby\u001b[0m \u001b[0mTorchCodec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     return load_with_torchcodec(\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mframe_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_torchcodec.py\u001b[0m in \u001b[0;36mload_with_torchcodec\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to create AudioDecoder for {uri}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# Get sample rate from metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to create AudioDecoder for /content/drive/MyDrive/meLaugh.mp3: Could not open input file: /content/drive/MyDrive/meLaugh.mp3 No such file or directory"
          ]
        }
      ],
      "source": [
        "def predict_attributes(audio_path: str):\n",
        "    # 1) Load & preprocess audio\n",
        "    wav = load_first_n_seconds(audio_path, TARGET_SR, MAX_SECONDS)  # (T,)\n",
        "    wav = wav.to(device)\n",
        "\n",
        "    # 2) Feature extraction\n",
        "    with torch.no_grad():\n",
        "        inputs = fe(wav, sampling_rate=TARGET_SR, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # 3) Encoder\n",
        "        hs = encoder(**inputs).last_hidden_state  # (1, T', d_model)\n",
        "\n",
        "        # 4) Temporal head\n",
        "        preds = head(hs)  # (1, num_targets)\n",
        "        preds = preds.squeeze(0).cpu().numpy().tolist()\n",
        "    return dict(zip(TARGET_KEYS, preds))\n",
        "\n",
        "# test some examples, including my own voice:\n",
        "result = predict_attributes(\"/content/drive/MyDrive/meHappy.mp3\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pYzoOFc19yF"
      },
      "outputs": [],
      "source": [
        "# prints the actual emotion in its expected range\n",
        "print(denormalize_range(result[\"Valence_best\"],\"Valence_best\"))\n",
        "print(denormalize_range(result[\"Arousal_best\"],'Arousal_best'))\n",
        "print(denormalize_range( result[\"Submissive_vs._Dominant_best\"],'Submissive_vs._Dominant_best'))\n",
        "print(denormalize_range(result[\"Serious_vs._Humorous_best\"],'Serious_vs._Humorous_best'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}