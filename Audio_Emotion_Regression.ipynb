{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chesseraf/Audio-emotion-recognition/blob/main/Audio_Emotion_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTmiRZfIcVwO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "DRIVE_MOUNTED = True\n",
        "if DRIVE_MOUNTED:\n",
        "  drive.mount('/content/drive')\n",
        "  SAVE_DIR = \"/content/drive/MyDrive/w2v2_temporal_head\"\n",
        "  DATA_DIR     = \"/content/drive/MyDrive/Colab_Drive_Files\"   # folder with .wav or .mp3 + matching .json\n",
        "else:\n",
        "  SAVE_DIR = \"./w2v2_temporal_head\"\n",
        "  DATA_DIR = \"./Data\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjw-m6TYMcT6"
      },
      "outputs": [],
      "source": [
        "!pip -q install torchcodec --index-url \"https://download.pytorch.org/whl/cu126\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gswZvVP-vnLv"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os, gc, torch, json, random, math, torchaudio, joblib\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoFeatureExtractor, AutoConfig, Wav2Vec2Model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rRfPgqacwLB"
      },
      "outputs": [],
      "source": [
        "os.environ[\"USE_TF\"] = \"0\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1Q_KHsTc5DQ"
      },
      "outputs": [],
      "source": [
        "# ==== USER SETTINGS ====\n",
        "\n",
        "TARGET_KEYS  = [\"Valence_best\",\"Arousal_best\",\"Submissive_vs._Dominant_best\", \"Serious_vs._Humorous_best\"]\n",
        "MODEL_NAME   = \"facebook/wav2vec2-base-960h\"  # small & stable; upgrade later if needed\n",
        "TARGET_SR    = 16_000\n",
        "MAX_SECONDS  = 12.0              # keep modest; you can try more later\n",
        "SEED         = 42\n",
        "\n",
        "# Training\n",
        "EPOCHS       = 50                # start small\n",
        "LR           = 1e-3             # higher LR since we train only a tiny head\n",
        "WEIGHT_DECAY = 0.007             # overfits with current data set size\n",
        "BATCH_SIZE   = 1                # keep at 1 for stability\n",
        "NUM_WORKERS  = 0                # 0 = no multiprocessing (stable on Colab)\n",
        "VAL_SPLIT    = 0.1              # if N>1, take ~10% for val\n",
        "MAX_FILES    = None             # set an int (e.g., 200) for a smoke test; None = all\n",
        "\n",
        "FEATURE_FILE_EXTENSION = '.npy' # encoder outputs saved as numpy tensors\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyqMiT2UDXD3"
      },
      "outputs": [],
      "source": [
        "# Normalization based on label ranges of different attributes\n",
        "ALL_KEYS_WITH_RANGE = {\n",
        "  \"Valence_best\": { \"min\": -3, \"max\": 3 },\n",
        "  \"Arousal_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Submissive_vs._Dominant_best\": { \"min\": -3, \"max\": 3 },\n",
        "  \"Age_best\": { \"min\": 0, \"max\": 6 },\n",
        "  \"Gender_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Serious_vs._Humorous_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Vulnerable_vs._Emotionally_Detached_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Confident_vs._Hesitant_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Warm_vs._Cold_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Monotone_vs._Expressive_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"High-Pitched_vs._Low-Pitched_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Soft_vs._Harsh_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Authenticity_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Recording_Quality_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Background_Noise_best\": { \"min\": 0, \"max\": 3 }\n",
        "}\n",
        "\n",
        "def normalize_range(value, attribute):\n",
        "    min_value = ALL_KEYS_WITH_RANGE[attribute][\"min\"]\n",
        "    max_value = ALL_KEYS_WITH_RANGE[attribute][\"max\"]\n",
        "    return (value - min_value) / (max_value - min_value)\n",
        "def denormalize_range(value, attribute):\n",
        "    min_value = ALL_KEYS_WITH_RANGE[attribute][\"min\"]\n",
        "    max_value = ALL_KEYS_WITH_RANGE[attribute][\"max\"]\n",
        "    return value * (max_value - min_value) + min_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "975c6090"
      },
      "outputs": [],
      "source": [
        "global num_mp3s_encoded\n",
        "global reused_encodings\n",
        "global presaved_encoding_found\n",
        "num_mp3s_encoded = 0\n",
        "reused_encodings = 0\n",
        "presaved_encoding_found = 0\n",
        "\n",
        "# Audio and emotions will be lazy initialized, and also store their output of the base encoder\n",
        "\n",
        "# Assuming normalize_range, load_first_n_seconds, TARGET_KEYS, TARGET_SR, MAX_SECONDS, fe, device are defined in the global scope\n",
        "\n",
        "class LazyAudioData:\n",
        "    def __init__(self, audio_path: str, json_path: str, existing_feature_path: str):\n",
        "        self._audio_path = audio_path\n",
        "        self._json_path = json_path\n",
        "        self._precalculated_feature_file = existing_feature_path\n",
        "        self._emotions = None  # To store cached emotions\n",
        "        self._wav = None       # To store cached waveform\n",
        "        self._encoded_features = None # To store cached encoded features\n",
        "\n",
        "    @property\n",
        "    def emotions(self) -> List[float]:\n",
        "\n",
        "        if self._emotions is None:\n",
        "            # Load and normalize emotions from JSON using global TARGET_KEYS and normalize_range\n",
        "            try:\n",
        "                emo = json.loads(Path(self._json_path).read_text(encoding=\"utf-8\")).get(\"emotion_annotation\", {})\n",
        "                labels = [normalize_range(float(emo[k]), k) for k in TARGET_KEYS]\n",
        "                if not all(np.isfinite(labels)):\n",
        "                    raise ValueError(\"Non-finite labels found after normalization\")\n",
        "                if isinstance(labels, torch.Tensor):\n",
        "                    labels = labels.unsqueeze(0)          # (1, D)\n",
        "                else:\n",
        "                    labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
        "                labels = labels.to(device)\n",
        "                self._emotions = labels\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading emotions from {self._json_path}: {e}\")\n",
        "                self._emotions = [] # Return empty list on error\n",
        "        return self._emotions\n",
        "\n",
        "    @property\n",
        "    def wav(self) -> torch.Tensor:\n",
        "        if self._wav is None:\n",
        "            # Load waveform using the existing global function and parameters\n",
        "            try:\n",
        "                self._wav = load_first_n_seconds(self._audio_path, TARGET_SR, MAX_SECONDS)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading audio from {self._audio_path}: {e}\")\n",
        "                self._wav = torch.empty(0) # Return empty tensor on error\n",
        "        return self._wav\n",
        "\n",
        "    @property\n",
        "    def encoded_features(self):\n",
        "        global num_mp3s_encoded\n",
        "        global reused_encodings\n",
        "        global presaved_encoding_found\n",
        "        if self._encoded_features is None:\n",
        "            # check if it has been computed and saved before\n",
        "            precalculated_feature = None\n",
        "\n",
        "            # was slow as implemented\n",
        "            if self._precalculated_feature_file is not None:\n",
        "                try:\n",
        "                  arr = np.load(Path(self._precalculated_feature_file))\n",
        "                  self._encoded_features = torch.from_numpy(arr).cuda()             # CPU tensor\n",
        "                  presaved_encoding_found += 1\n",
        "                  if presaved_encoding_found % 100 == 0:\n",
        "                    print(f\"Found {presaved_encoding_found} presaved encodings\")\n",
        "                  return self._encoded_features\n",
        "                except Exception as e:\n",
        "                  print(f\"Warning: Error loading precalculated features from {self._precalculated_feature_file}: {e}\")\n",
        "\n",
        "            # Ensure wav is loaded first\n",
        "            if self._wav is None:\n",
        "                _ = self.wav # Trigger wav loading\n",
        "            if self._wav is not None and self._wav.numel() > 0:\n",
        "                # Process wav through feature extractor (global `fe`)\n",
        "                # Feat should be moved to device for consistency with batch_to_inputs\n",
        "                feat = fe(self._wav, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=\"do_not_pad\")\n",
        "\n",
        "                # THESE ARE ENCODER INPUTS, NOT FEATURES\n",
        "                inputs = {k: v.to(device) for k, v in feat.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    out = encoder(input_values=inputs[\"input_values\"])\n",
        "                    self._encoded_features = out.last_hidden_state  # (B, T', d_model)\n",
        "\n",
        "                    # Save the encoded features to a file for future runs\n",
        "                    feature_path = Path(self._audio_path).with_suffix(FEATURE_FILE_EXTENSION)\n",
        "                    np.save(feature_path, self._encoded_features.cpu().numpy())\n",
        "                # dont keep the large wavs in ram\n",
        "                del self._wav\n",
        "                self._wav = None\n",
        "                num_mp3s_encoded += 1\n",
        "                if num_mp3s_encoded % 100 == 0:\n",
        "                    print(f\"Processed {num_mp3s_encoded} mp3s\")\n",
        "        else:\n",
        "          reused_encodings += 1\n",
        "          if reused_encodings == 1:\n",
        "            print(f\"Reused an encoding!\")\n",
        "\n",
        "        return self._encoded_features\n",
        "\n",
        "print(\"LazyAudioData class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4YjYyNSc-HN"
      },
      "outputs": [],
      "source": [
        "def collect_pairs(data_dir: str, target_keys: List[str], limit: Optional[int]=None):\n",
        "    root = Path(data_dir)\n",
        "    files = sorted(root.rglob(\"*.mp3\"))\n",
        "    items = []\n",
        "    for mp3 in files:\n",
        "        j = mp3.with_suffix(\".json\")\n",
        "        if not j.exists():\n",
        "            continue\n",
        "        precalculated_feature_file = mp3.with_suffix(FEATURE_FILE_EXTENSION)\n",
        "        if not precalculated_feature_file.exists():\n",
        "            calculatedFile = None\n",
        "        else:\n",
        "            calculatedFile = str(precalculated_feature_file)\n",
        "\n",
        "        items.append(LazyAudioData(str(mp3), str(j), calculatedFile))\n",
        "    if not items:\n",
        "        raise RuntimeError(\"No usable (audio,json) pairs found.\")\n",
        "    return items\n",
        "\n",
        "items = collect_pairs(DATA_DIR, TARGET_KEYS, limit=MAX_FILES)\n",
        "random.shuffle(items)\n",
        "\n",
        "# Split\n",
        "n = len(items)\n",
        "if n == 1:\n",
        "    train_items, val_items = items, []\n",
        "else:\n",
        "    n_val = min(max(1, int(n * VAL_SPLIT)), n-1)\n",
        "    val_items, train_items = items[:n_val], items[n_val:]\n",
        "\n",
        "print(f\"pairs total={n}  train={len(train_items)}  val={len(val_items)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXNs3B1_dBmp"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = int(TARGET_SR * MAX_SECONDS)\n",
        "_resamplers: Dict[Tuple[int,int], torchaudio.transforms.Resample] = {}\n",
        "\n",
        "def load_first_n_seconds(path: str, target_sr: int, max_seconds: float) -> torch.Tensor:\n",
        "    # infer original SR without decoding full file\n",
        "    try:\n",
        "        info = torchaudio.info(path)\n",
        "        orig_sr = info.sample_rate\n",
        "    except Exception:\n",
        "        _, orig_sr = torchaudio.load(path, frame_offset=0, num_frames=1024)\n",
        "    frames = int(orig_sr * max_seconds)\n",
        "\n",
        "    # read only that window\n",
        "    wav, sr = torchaudio.load(path, frame_offset=0, num_frames=frames)  # (C, T<=frames)\n",
        "\n",
        "    # mono\n",
        "    if wav.shape[0] > 1:\n",
        "        wav = wav.mean(0, keepdim=True)\n",
        "    # resample minimal window\n",
        "    if sr != target_sr:\n",
        "        key = (sr, target_sr)\n",
        "        if key not in _resamplers:\n",
        "            _resamplers[key] = torchaudio.transforms.Resample(sr, target_sr)\n",
        "        wav = _resamplers[key](wav)\n",
        "    wav = wav.squeeze(0)\n",
        "\n",
        "    # truncate/pad to EXACT MAX_LEN (so we can use padding=\"do_not_pad\")\n",
        "    if wav.numel() > MAX_LEN:\n",
        "        wav = wav[:MAX_LEN]\n",
        "    if wav.numel() < MAX_LEN:\n",
        "        wav = torch.nn.functional.pad(wav, (0, MAX_LEN - wav.numel()))\n",
        "\n",
        "    # peak normalize\n",
        "    wav = wav / (wav.abs().max() + 1e-9)\n",
        "    return wav\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEMDizHKdLx1"
      },
      "outputs": [],
      "source": [
        "class PathDataset(Dataset):\n",
        "    def __init__(self, items: List[Dict[str,Any]]):\n",
        "        self.items = items\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.items[idx]\n",
        "        return {\"encodedFeatures\": ex.encoded_features, \"labels\": ex.emotions}\n",
        "\n",
        "train_ds = PathDataset(train_items)\n",
        "val_ds   = PathDataset(val_items) if len(val_items) else None\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
        "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False) if val_ds else None\n",
        "\n",
        "print(\"Loaders ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWGws16-dM0O"
      },
      "outputs": [],
      "source": [
        "# Feature extractor & encoder (frozen)\n",
        "fe = AutoFeatureExtractor.from_pretrained(MODEL_NAME, sampling_rate=TARGET_SR)\n",
        "enc_cfg = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=False)\n",
        "encoder = Wav2Vec2Model.from_pretrained(MODEL_NAME, config=enc_cfg).to(device)\n",
        "encoder.eval()\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Tiny temporal head: GRU + attention pooling -> 3 outputs\n",
        "class TemporalHead(nn.Module):\n",
        "    def __init__(self, d_model=768, hidden=128, out_dim=len(TARGET_KEYS)):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(d_model, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Linear(2*hidden, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.LayerNorm(2*hidden),\n",
        "            nn.Linear(2*hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, hs):                 # hs: (B, T', d_model)\n",
        "        z, _ = self.gru(hs)                # (B, T', 2H)\n",
        "        a = self.att(z).squeeze(-1)        # (B, T')\n",
        "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
        "        pooled = (w * z).sum(dim=1)        # (B, 2H)\n",
        "        return self.out(pooled)            # (B, out_dim)\n",
        "\n",
        "num_labels = len(TARGET_KEYS)\n",
        "head = TemporalHead(d_model=encoder.config.hidden_size, hidden=128, out_dim=num_labels).to(device)\n",
        "\n",
        "# Only head is trainable\n",
        "opt = torch.optim.AdamW(head.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "mse = nn.MSELoss()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
        "\n",
        "print(\"Encoder frozen. Trainable head params:\",\n",
        "      sum(p.numel() for p in head.parameters() if p.requires_grad))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtM5rEJgYJKT"
      },
      "outputs": [],
      "source": [
        "# run partial validation report every so often\n",
        "global training_count_report # every so many training samples make and print a report\n",
        "training_count_report = 410\n",
        "global val_count_to_report\n",
        "val_count_to_report = 400\n",
        "global report_num\n",
        "report_num = 0\n",
        "def run_val_report(numsamples=val_count_to_report, mini_train_loss=None):\n",
        "  global report_num\n",
        "  report_num += 1\n",
        "  with torch.no_grad():\n",
        "    va_loss = run_epoch(val_loader, train=False, limit_samples=numsamples)\n",
        "    print(f\"report {report_num} | val MSE={va_loss:.4f}\")\n",
        "    if mini_train_loss is not None:\n",
        "      print(f\"report {report_num} | train MSE={mini_train_loss/training_count_report:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4eXYz08dR3g"
      },
      "outputs": [],
      "source": [
        "def run_epoch(loader, train=True, limit_samples=None):\n",
        "    if train:\n",
        "        head.train()\n",
        "    else:\n",
        "        head.eval()\n",
        "    total_loss = 0.0\n",
        "    mini_report_loss = 0.0\n",
        "    sample_num = 0\n",
        "    # batch is only one sample\n",
        "    for batch in loader:\n",
        "        sample_num += 1\n",
        "        if limit_samples is not None and sample_num > limit_samples:\n",
        "            break\n",
        "        # intermediate reports can be run when data set is large and slow\n",
        "        # if sample_num % training_count_report == 0:\n",
        "        #   run_val_report(mini_train_loss=mini_report_loss)\n",
        "        #   mini_report_loss = 0\n",
        "        #   if train: # Ensure head is back in train mode after validation report\n",
        "        #     head.train()\n",
        "\n",
        "        hs, labels = batch[\"encodedFeatures\"].squeeze(0), batch[\"labels\"]\n",
        "        with torch.amp.autocast('cuda', enabled=(device.type==\"cuda\")):\n",
        "            preds = head(hs)\n",
        "            # average of the 4 emotion's square errors\n",
        "            loss = mse(preds, labels)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "        cLoss = loss.item() * labels.size(0)\n",
        "        total_loss += cLoss\n",
        "        mini_report_loss += cLoss\n",
        "\n",
        "    return total_loss / max(1, sample_num)\n",
        "with torch.no_grad():\n",
        "     va_loss = run_epoch(val_loader, train=False)\n",
        "     print(f\"Epoch {0}/{EPOCHS} | val MSE={va_loss:.4f}\")\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss = run_epoch(train_loader, train=True)\n",
        "    if val_loader:\n",
        "        with torch.no_grad():\n",
        "            va_loss = run_epoch(val_loader, train=False)\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f} | val MSE={va_loss:.4f}\")\n",
        "        if va_loss < 0.0041:\n",
        "            break\n",
        "    else:\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsUmON-SwMiY"
      },
      "outputs": [],
      "source": [
        "print(num_mp3s_encoded)\n",
        "print(reused_encodings)\n",
        "print(presaved_encoding_found)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsSrZpMJdWRl"
      },
      "outputs": [],
      "source": [
        "def ccc(y_true, y_pred):\n",
        "    y = np.asarray(y_true, np.float64)\n",
        "    x = np.asarray(y_pred, np.float64)\n",
        "    vx, vy = x.var(), y.var()\n",
        "    mx, my = x.mean(), y.mean()\n",
        "    cov = ((x - mx) * (y - my)).mean()\n",
        "    denom = vx + vy + (mx - my)**2\n",
        "    return float(2 * cov / denom) if denom > 0 else 0.0\n",
        "\n",
        "def evaluate_full(loader):\n",
        "    y_true, y_pred = [], []\n",
        "    head.eval()\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            hs, labels = batch[\"encodedFeatures\"].squeeze(0), batch[\"labels\"]\n",
        "            preds = head(hs)\n",
        "            y_true.append(labels.detach().cpu().numpy())\n",
        "            y_pred.append(preds.detach().cpu().numpy())\n",
        "            torch.cuda.empty_cache()\n",
        "    y_true = np.array(y_true).squeeze(1)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    true_sq = np.array(y_true).squeeze()\n",
        "    pred_sq = np.array(y_pred).squeeze()\n",
        "\n",
        "    mean_true = true_sq.mean(axis=0)\n",
        "    mean_pred = pred_sq.mean(axis=0)\n",
        "    std_true = true_sq.std(axis=0)\n",
        "    std_pred = pred_sq.std(axis=0)\n",
        "    print(\"Keys:            \", TARGET_KEYS)\n",
        "    print(\"Validation mean: \", mean_true)\n",
        "    print(\"Predicted mean:  \", mean_pred)\n",
        "    print(\"Validation std: \", std_true)\n",
        "    print(\"Predicted std:  \", std_pred)\n",
        "\n",
        "    Y = np.concatenate(y_true, axis=0)\n",
        "    P = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    mae = mean_absolute_error(Y, P, multioutput=\"raw_values\")\n",
        "    mse = mean_squared_error(Y, P, multioutput=\"raw_values\")\n",
        "    metrics = {\n",
        "        \"MAE_macro\": float(mae.mean()),\n",
        "        \"MSE_macro\": float(mse.mean()),\n",
        "    }\n",
        "    for i,k in enumerate(TARGET_KEYS):\n",
        "        metrics[f\"MAE_{k}\"] = float(mae[i])\n",
        "        metrics[f\"MSE_{k}\"] = float(mse[i])\n",
        "        metrics[f\"CCC_{k}\"] = ccc(Y[:,i], P[:,i])\n",
        "    return metrics\n",
        "\n",
        "if val_loader and len(val_ds) > 0:\n",
        "    metrics = evaluate_full(val_loader)\n",
        "    print(\"Validation metrics:\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "else:\n",
        "    print(\"No validation split; skipped metrics.\")\n",
        "\n",
        "if train_loader and len(train_ds) > 0:\n",
        "    metrics = evaluate_full(train_loader)\n",
        "    print(\"Training metrics:\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "else:\n",
        "    print(\"No training split; skipped metrics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da-sMGLJdbMU"
      },
      "outputs": [],
      "source": [
        "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save torch head\n",
        "torch.save(head.state_dict(), f\"{SAVE_DIR}/temporal_head.pt\")\n",
        "# Save config to rebuild pipeline later\n",
        "json.dump({\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"target_sr\": TARGET_SR,\n",
        "    \"max_seconds\": MAX_SECONDS,\n",
        "    \"target_keys\": TARGET_KEYS,\n",
        "    \"head\": {\"d_model\": int(encoder.config.hidden_size), \"hidden\": 128, \"out_dim\": len(TARGET_KEYS)}\n",
        "}, open(f\"{SAVE_DIR}/config.json\",\"w\"))\n",
        "\n",
        "print(\"Saved to\", SAVE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMMkk3W-_ztR"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1) Load config\n",
        "cfg_path = f\"{SAVE_DIR}/config.json\"\n",
        "state_path = f\"{SAVE_DIR}/temporal_head.pt\"\n",
        "\n",
        "with open(cfg_path, \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "MODEL_NAME  = cfg[\"model_name\"]\n",
        "TARGET_SR   = cfg[\"target_sr\"]\n",
        "MAX_SECONDS = cfg[\"max_seconds\"]\n",
        "TARGET_KEYS = cfg[\"target_keys\"]\n",
        "head_cfg    = cfg[\"head\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SDEfok8_2Rj"
      },
      "outputs": [],
      "source": [
        "# Reload frozen base encoder\n",
        "fe = AutoFeatureExtractor.from_pretrained(MODEL_NAME, sampling_rate=TARGET_SR)\n",
        "enc_cfg = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=False)\n",
        "encoder = Wav2Vec2Model.from_pretrained(MODEL_NAME, config=enc_cfg).to(device)\n",
        "encoder.eval()\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Recreate the TemporalHead with same dimensions from config\n",
        "class TemporalHead(nn.Module):\n",
        "    def __init__(self, d_model=768, hidden=128, out_dim=len(TARGET_KEYS)):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(d_model, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Linear(2*hidden, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.LayerNorm(2*hidden),\n",
        "            nn.Linear(2*hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, hs):                 # hs: (B, T', d_model)\n",
        "        z, _ = self.gru(hs)                # (B, T', 2H)\n",
        "        a = self.att(z).squeeze(-1)        # (B, T')\n",
        "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
        "        pooled = (w * z).sum(dim=1)        # (B, 2H)\n",
        "        return self.out(pooled)            # (B, out_dim)\n",
        "\n",
        "\n",
        "head = TemporalHead(\n",
        "    d_model=head_cfg[\"d_model\"],\n",
        "    hidden=head_cfg[\"hidden\"],\n",
        "    out_dim=head_cfg[\"out_dim\"],\n",
        ").to(device)\n",
        "\n",
        "# Load trained weights\n",
        "state = torch.load(state_path, map_location=device)\n",
        "head.load_state_dict(state)\n",
        "head.eval()\n",
        "\n",
        "print(\"Model reloaded and ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-WXvzQS_4xj"
      },
      "outputs": [],
      "source": [
        "def predict_attributes(audio_path: str):\n",
        "    # 1) Load & preprocess audio\n",
        "    wav = load_first_n_seconds(audio_path, TARGET_SR, MAX_SECONDS)  # (T,)\n",
        "    wav = wav.to(device)\n",
        "\n",
        "    # 2) Feature extraction\n",
        "    with torch.no_grad():\n",
        "        inputs = fe(wav, sampling_rate=TARGET_SR, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # 3) Encoder\n",
        "        hs = encoder(**inputs).last_hidden_state  # (1, T', d_model)\n",
        "\n",
        "        # 4) Temporal head\n",
        "        preds = head(hs)  # (1, num_targets)\n",
        "        preds = preds.squeeze(0).cpu().numpy().tolist()\n",
        "    return dict(zip(TARGET_KEYS, preds))\n",
        "\n",
        "# test some examples, including my own voice:\n",
        "result = predict_attributes(\"/content/drive/MyDrive/meHappy.mp3\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pYzoOFc19yF"
      },
      "outputs": [],
      "source": [
        "# prints the actual emotion in its expected range\n",
        "print(denormalize_range(result[\"Valence_best\"],\"Valence_best\"))\n",
        "print(denormalize_range(result[\"Arousal_best\"],'Arousal_best'))\n",
        "print(denormalize_range( result[\"Submissive_vs._Dominant_best\"],'Submissive_vs._Dominant_best'))\n",
        "print(denormalize_range(result[\"Serious_vs._Humorous_best\"],'Serious_vs._Humorous_best'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
