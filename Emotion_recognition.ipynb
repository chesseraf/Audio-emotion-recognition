{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTmiRZfIcVwO",
        "outputId": "0ea7a175-85ef-40b4-ebfb-101030bbe8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjw-m6TYMcT6",
        "outputId": "844a6d08-bc17-47e3-dde1-35e4bf5dc813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install torchcodec --index-url \"https://download.pytorch.org/whl/cu126\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gswZvVP-vnLv"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os, gc, torch, json, random, math, torchaudio, joblib\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoFeatureExtractor, AutoConfig, Wav2Vec2Model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rRfPgqacwLB",
        "outputId": "4a93f472-edb9-4ec1-e613-51304e4b0cd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.9.0+cu126 | CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"USE_TF\"] = \"0\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P1Q_KHsTc5DQ"
      },
      "outputs": [],
      "source": [
        "# ==== USER SETTINGS ====\n",
        "DRIVE_MOUNTED = True\n",
        "DATA_DIR     = \"/content/drive/MyDrive/Colab_Drive_Files/Worker1_nov\"   # folder with .wav or .mp3 + matching .json\n",
        "if not DRIVE_MOUNTED:\n",
        "    DATA_DIR = \"./Data\"\n",
        "TARGET_KEYS  = [\"Valence_best\",\"Arousal_best\",\"Submissive_vs._Dominant_best\", \"Serious_vs._Humorous_best\"]\n",
        "MODEL_NAME   = \"facebook/wav2vec2-base-960h\"  # small & stable; upgrade later if needed\n",
        "TARGET_SR    = 16_000\n",
        "MAX_SECONDS  = 12.0              # keep modest; you can try more later\n",
        "SEED         = 42\n",
        "\n",
        "# Training\n",
        "EPOCHS       = 1000                # start small\n",
        "LR           = 1e-3             # higher LR since we train only a tiny head\n",
        "WEIGHT_DECAY = 0.007             # overfits with current data set size\n",
        "BATCH_SIZE   = 1                # keep at 1 for stability\n",
        "NUM_WORKERS  = 0                # 0 = no multiprocessing (stable on Colab)\n",
        "VAL_SPLIT    = 0.1              # if N>1, take ~10% for val\n",
        "MAX_FILES    = None             # set an int (e.g., 200) for a smoke test; None = all\n",
        "\n",
        "FEATURE_FILE_EXTENSION = '.npy' # encoder outputs saved as numpy tensors\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZyqMiT2UDXD3"
      },
      "outputs": [],
      "source": [
        "# Normalization based on label ranges of different attributes\n",
        "ALL_KEYS_WITH_RANGE = {\n",
        "  \"Valence_best\": { \"min\": -3, \"max\": 3 },\n",
        "  \"Arousal_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Submissive_vs._Dominant_best\": { \"min\": -3, \"max\": 3 },\n",
        "  \"Age_best\": { \"min\": 0, \"max\": 6 },\n",
        "  \"Gender_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Serious_vs._Humorous_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Vulnerable_vs._Emotionally_Detached_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Confident_vs._Hesitant_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Warm_vs._Cold_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Monotone_vs._Expressive_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"High-Pitched_vs._Low-Pitched_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Soft_vs._Harsh_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Authenticity_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Recording_Quality_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Background_Noise_best\": { \"min\": 0, \"max\": 3 }\n",
        "}\n",
        "\n",
        "def normalize_range(value, attribute):\n",
        "    min_value = ALL_KEYS_WITH_RANGE[attribute][\"min\"]\n",
        "    max_value = ALL_KEYS_WITH_RANGE[attribute][\"max\"]\n",
        "    return (value - min_value) / (max_value - min_value)\n",
        "def denormalize_range(value, attribute):\n",
        "    min_value = ALL_KEYS_WITH_RANGE[attribute][\"min\"]\n",
        "    max_value = ALL_KEYS_WITH_RANGE[attribute][\"max\"]\n",
        "    return value * (max_value - min_value) + min_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "975c6090",
        "outputId": "0bf4ebef-ca12-45be-a23a-b5190759a7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LazyAudioData class defined.\n"
          ]
        }
      ],
      "source": [
        "global num_mp3s_encoded\n",
        "global reused_encodings\n",
        "global presaved_encoding_found\n",
        "num_mp3s_encoded = 0\n",
        "reused_encodings = 0\n",
        "presaved_encoding_found = 0\n",
        "\n",
        "# Audio and emotions will be lazy initialized, and also store their output of the base encoder\n",
        "\n",
        "# Assuming normalize_range, load_first_n_seconds, TARGET_KEYS, TARGET_SR, MAX_SECONDS, fe, device are defined in the global scope\n",
        "\n",
        "class LazyAudioData:\n",
        "    def __init__(self, audio_path: str, json_path: str, existing_feature_path: str):\n",
        "        self._audio_path = audio_path\n",
        "        self._json_path = json_path\n",
        "        self._precalculated_feature_file = existing_feature_path\n",
        "        self._emotions = None  # To store cached emotions\n",
        "        self._wav = None       # To store cached waveform\n",
        "        self._encoded_features = None # To store cached encoded features\n",
        "\n",
        "    @property\n",
        "    def emotions(self) -> List[float]:\n",
        "\n",
        "        if self._emotions is None:\n",
        "            # Load and normalize emotions from JSON using global TARGET_KEYS and normalize_range\n",
        "            try:\n",
        "                emo = json.loads(Path(self._json_path).read_text(encoding=\"utf-8\")).get(\"emotion_annotation\", {})\n",
        "                labels = [normalize_range(float(emo[k]), k) for k in TARGET_KEYS]\n",
        "                if not all(np.isfinite(labels)):\n",
        "                    raise ValueError(\"Non-finite labels found after normalization\")\n",
        "                if isinstance(labels, torch.Tensor):\n",
        "                    labels = labels.unsqueeze(0)          # (1, D)\n",
        "                else:\n",
        "                    labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
        "                labels = labels.to(device)\n",
        "                self._emotions = labels\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading emotions from {self._json_path}: {e}\")\n",
        "                self._emotions = [] # Return empty list on error\n",
        "        return self._emotions\n",
        "\n",
        "    @property\n",
        "    def wav(self) -> torch.Tensor:\n",
        "        if self._wav is None:\n",
        "            # Load waveform using the existing global function and parameters\n",
        "            try:\n",
        "                self._wav = load_first_n_seconds(self._audio_path, TARGET_SR, MAX_SECONDS)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading audio from {self._audio_path}: {e}\")\n",
        "                self._wav = torch.empty(0) # Return empty tensor on error\n",
        "        return self._wav\n",
        "\n",
        "    @property\n",
        "    def encoded_features(self):\n",
        "        global num_mp3s_encoded\n",
        "        global reused_encodings\n",
        "        global presaved_encoding_found\n",
        "        if self._encoded_features is None:\n",
        "            # check if it has been computed and saved before\n",
        "            precalculated_feature = None\n",
        "\n",
        "            # was slow as implemented\n",
        "            if self._precalculated_feature_file is not None:\n",
        "                try:\n",
        "                  arr = np.load(Path(self._precalculated_feature_file))\n",
        "                  self._encoded_features = torch.from_numpy(arr).cuda()             # CPU tensor\n",
        "                  presaved_encoding_found += 1\n",
        "                  if presaved_encoding_found % 100 == 0:\n",
        "                    print(f\"Found {presaved_encoding_found} presaved encodings\")\n",
        "                  return self._encoded_features\n",
        "                except Exception as e:\n",
        "                  print(f\"Warning: Error loading precalculated features from {self._precalculated_feature_file}: {e}\")\n",
        "\n",
        "            # Ensure wav is loaded first\n",
        "            if self._wav is None:\n",
        "                _ = self.wav # Trigger wav loading\n",
        "            if self._wav is not None and self._wav.numel() > 0:\n",
        "                # Process wav through feature extractor (global `fe`)\n",
        "                # Feat should be moved to device for consistency with batch_to_inputs\n",
        "                feat = fe(self._wav, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=\"do_not_pad\")\n",
        "\n",
        "                # THESE ARE ENCODER INPUTS, NOT FEATURES\n",
        "                inputs = {k: v.to(device) for k, v in feat.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    out = encoder(input_values=inputs[\"input_values\"])\n",
        "                    self._encoded_features = out.last_hidden_state  # (B, T', d_model)\n",
        "\n",
        "                    # Save the encoded features to a file for future runs\n",
        "                    feature_path = Path(self._audio_path).with_suffix(FEATURE_FILE_EXTENSION)\n",
        "                    np.save(feature_path, self._encoded_features.cpu().numpy())\n",
        "                # dont keep the large wavs in ram\n",
        "                del self._wav\n",
        "                self._wav = None\n",
        "                num_mp3s_encoded += 1\n",
        "                if num_mp3s_encoded % 100 == 0:\n",
        "                    print(f\"Processed {num_mp3s_encoded} mp3s\")\n",
        "        else:\n",
        "          reused_encodings += 1\n",
        "          if reused_encodings == 1:\n",
        "            print(f\"Reused an encoding!\")\n",
        "\n",
        "        return self._encoded_features\n",
        "\n",
        "print(\"LazyAudioData class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4YjYyNSc-HN",
        "outputId": "81a4ddb2-e9fc-486d-f099-1927236c0ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pairs total=2404  train=2164  val=240\n"
          ]
        }
      ],
      "source": [
        "def collect_pairs(data_dir: str, target_keys: List[str], limit: Optional[int]=None):\n",
        "    root = Path(data_dir)\n",
        "    files = sorted(root.rglob(\"*.mp3\"))\n",
        "    items = []\n",
        "    for mp3 in files:\n",
        "        j = mp3.with_suffix(\".json\")\n",
        "        if not j.exists():\n",
        "            continue\n",
        "        precalculated_feature_file = mp3.with_suffix(FEATURE_FILE_EXTENSION)\n",
        "        if not precalculated_feature_file.exists():\n",
        "            calculatedFile = None\n",
        "        else:\n",
        "            calculatedFile = str(precalculated_feature_file)\n",
        "\n",
        "        items.append(LazyAudioData(str(mp3), str(j), calculatedFile))\n",
        "    if not items:\n",
        "        raise RuntimeError(\"No usable (audio,json) pairs found.\")\n",
        "    return items\n",
        "\n",
        "items = collect_pairs(DATA_DIR, TARGET_KEYS, limit=MAX_FILES)\n",
        "random.shuffle(items)\n",
        "\n",
        "# Split\n",
        "n = len(items)\n",
        "if n == 1:\n",
        "    train_items, val_items = items, []\n",
        "else:\n",
        "    n_val = min(max(1, int(n * VAL_SPLIT)), n-1)\n",
        "    val_items, train_items = items[:n_val], items[n_val:]\n",
        "\n",
        "print(f\"pairs total={n}  train={len(train_items)}  val={len(val_items)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DXNs3B1_dBmp"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = int(TARGET_SR * MAX_SECONDS)\n",
        "_resamplers: Dict[Tuple[int,int], torchaudio.transforms.Resample] = {}\n",
        "\n",
        "def load_first_n_seconds(path: str, target_sr: int, max_seconds: float) -> torch.Tensor:\n",
        "    # infer original SR without decoding full file\n",
        "    try:\n",
        "        info = torchaudio.info(path)\n",
        "        orig_sr = info.sample_rate\n",
        "    except Exception:\n",
        "        _, orig_sr = torchaudio.load(path, frame_offset=0, num_frames=1024)\n",
        "    frames = int(orig_sr * max_seconds)\n",
        "\n",
        "    # read only that window\n",
        "    wav, sr = torchaudio.load(path, frame_offset=0, num_frames=frames)  # (C, T<=frames)\n",
        "\n",
        "    # mono\n",
        "    if wav.shape[0] > 1:\n",
        "        wav = wav.mean(0, keepdim=True)\n",
        "    # resample minimal window\n",
        "    if sr != target_sr:\n",
        "        key = (sr, target_sr)\n",
        "        if key not in _resamplers:\n",
        "            _resamplers[key] = torchaudio.transforms.Resample(sr, target_sr)\n",
        "        wav = _resamplers[key](wav)\n",
        "    wav = wav.squeeze(0)\n",
        "\n",
        "    # truncate/pad to EXACT MAX_LEN (so we can use padding=\"do_not_pad\")\n",
        "    if wav.numel() > MAX_LEN:\n",
        "        wav = wav[:MAX_LEN]\n",
        "    if wav.numel() < MAX_LEN:\n",
        "        wav = torch.nn.functional.pad(wav, (0, MAX_LEN - wav.numel()))\n",
        "\n",
        "    # peak normalize\n",
        "    wav = wav / (wav.abs().max() + 1e-9)\n",
        "    return wav\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEMDizHKdLx1",
        "outputId": "3750bea9-e043-4f62-d0ae-45def2060b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders ready.\n"
          ]
        }
      ],
      "source": [
        "class PathDataset(Dataset):\n",
        "    def __init__(self, items: List[Dict[str,Any]]):\n",
        "        self.items = items\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.items[idx]\n",
        "        return {\"encodedFeatures\": ex.encoded_features, \"labels\": ex.emotions}\n",
        "\n",
        "train_ds = PathDataset(train_items)\n",
        "val_ds   = PathDataset(val_items) if len(val_items) else None\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
        "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False) if val_ds else None\n",
        "\n",
        "print(\"Loaders ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324,
          "referenced_widgets": [
            "b4d7a9913f8141ba83aa1e1d8d0888cb",
            "6318342a707c495ca5ec32ff1630c55a",
            "0a16facb1f79470dbbaa46e680fde028",
            "d8f6f12d7dd64cd8afc46c070fb610e8",
            "2bbe872928fd4e10b3a26cfa952b35a9",
            "0bf6aebe099147aeb9b534e3942af3c5",
            "da877c550ec34b51a3977db19bc9f71b",
            "f6d4c26e875e407988353f8f90fedba8",
            "4aca9e0179524f159a67bc49aabe0496",
            "09eac39f2bd9497cbd8e2cf83c17b751",
            "9717efeb0e064174b5dfbf9d334ce5c8",
            "fb94b89a413f48cb8a1195a94b136ea9",
            "fc217e6120044f9ab34b88cb171bf939",
            "bac8160b904e455cba14fb962b1c0662",
            "fe922d9d703b44159cbadfbeb030a66f",
            "3ea54c72237241a2ba623874665756c5",
            "e80bebb204e14ea68655cae31a3292f3",
            "7cd72bcb20014383b4a2f9250add96d9",
            "183e2ce8e719467ba7c0e6068354e048",
            "79a7d6b2bef0432383277500db498942",
            "de30a400feba4ed3bcc7ea5b438e18fd",
            "e451464e484140bd9aa9fcb2ea3c4845",
            "02dc394f373344739697cebed1a9324c",
            "45c25406802943d8aff52f18866f45f1",
            "aa9f105c64f8463180c0c49955a5accd",
            "8427cbfdd1d74fb0a4438764fa3a20ca",
            "8dced1de93d24a05a71ee5d728ba3e0c",
            "03f432a392ec414aa55aebf9ba829905",
            "8e81c1c47d2a4dc486f81856608b8613",
            "4ed6d929c17e41a081630f0890b0434d",
            "7c3c21311c3d437aae74f00ae3c3960b",
            "85dc542a0b0541209589dbd334ae79e4",
            "9a9899a35d954cebbc9a3673fafc67c6"
          ]
        },
        "id": "qWGws16-dM0O",
        "outputId": "b8ecda19-713b-4149-c58e-938e412f01c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4d7a9913f8141ba83aa1e1d8d0888cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb94b89a413f48cb8a1195a94b136ea9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02dc394f373344739697cebed1a9324c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder frozen. Trainable head params: 724229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2202145022.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n"
          ]
        }
      ],
      "source": [
        "# Feature extractor & encoder (frozen)\n",
        "fe = AutoFeatureExtractor.from_pretrained(MODEL_NAME, sampling_rate=TARGET_SR)\n",
        "enc_cfg = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=False)\n",
        "encoder = Wav2Vec2Model.from_pretrained(MODEL_NAME, config=enc_cfg).to(device)\n",
        "encoder.eval()\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Tiny temporal head: GRU + attention pooling -> 3 outputs\n",
        "class TemporalHead(nn.Module):\n",
        "    def __init__(self, d_model=768, hidden=128, out_dim=len(TARGET_KEYS)):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(d_model, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Linear(2*hidden, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.LayerNorm(2*hidden),\n",
        "            nn.Linear(2*hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, hs):                 # hs: (B, T', d_model)\n",
        "        z, _ = self.gru(hs)                # (B, T', 2H)\n",
        "        a = self.att(z).squeeze(-1)        # (B, T')\n",
        "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
        "        pooled = (w * z).sum(dim=1)        # (B, 2H)\n",
        "        return self.out(pooled)            # (B, out_dim)\n",
        "\n",
        "num_labels = len(TARGET_KEYS)\n",
        "head = TemporalHead(d_model=encoder.config.hidden_size, hidden=128, out_dim=num_labels).to(device)\n",
        "\n",
        "# Only head is trainable\n",
        "opt = torch.optim.AdamW(head.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "mse = nn.MSELoss()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
        "\n",
        "print(\"Encoder frozen. Trainable head params:\",\n",
        "      sum(p.numel() for p in head.parameters() if p.requires_grad))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WtM5rEJgYJKT"
      },
      "outputs": [],
      "source": [
        "# run partial validation report every so often\n",
        "global training_count_report # every so many training samples make and print a report\n",
        "training_count_report = 410\n",
        "global val_count_to_report\n",
        "val_count_to_report = 400\n",
        "global report_num\n",
        "report_num = 0\n",
        "def run_val_report(numsamples=val_count_to_report, mini_train_loss=None):\n",
        "  global report_num\n",
        "  report_num += 1\n",
        "  with torch.no_grad():\n",
        "    va_loss = run_epoch(val_loader, train=False, limit_samples=numsamples)\n",
        "    print(f\"report {report_num} | val MSE={va_loss:.4f}\")\n",
        "    if mini_train_loss is not None:\n",
        "      print(f\"report {report_num} | train MSE={mini_train_loss/training_count_report:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4eXYz08dR3g",
        "outputId": "9095bdd0-fa82-46ee-bb38-ec4ae3754687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:634: UserWarning: Using a target size (torch.Size([1, 1, 4])) that is different to the input size (torch.Size([1, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 presaved encodings\n",
            "Found 200 presaved encodings\n",
            "Epoch 0/1000 | val MSE=0.0610\n",
            "Found 300 presaved encodings\n",
            "Found 400 presaved encodings\n",
            "Found 500 presaved encodings\n",
            "Found 600 presaved encodings\n",
            "Found 700 presaved encodings\n",
            "Found 800 presaved encodings\n",
            "Found 900 presaved encodings\n",
            "Found 1000 presaved encodings\n",
            "Found 1100 presaved encodings\n",
            "Found 1200 presaved encodings\n",
            "Found 1300 presaved encodings\n",
            "Found 1400 presaved encodings\n",
            "Found 1500 presaved encodings\n",
            "Found 1600 presaved encodings\n",
            "Found 1700 presaved encodings\n",
            "Found 1800 presaved encodings\n",
            "Found 1900 presaved encodings\n",
            "Found 2000 presaved encodings\n",
            "Found 2100 presaved encodings\n",
            "Found 2200 presaved encodings\n",
            "Found 2300 presaved encodings\n",
            "Found 2400 presaved encodings\n",
            "Reused 1 encodings\n",
            "Epoch 1/1000 | train MSE=0.0165 | val MSE=0.0077\n",
            "Epoch 2/1000 | train MSE=0.0100 | val MSE=0.0087\n",
            "Epoch 3/1000 | train MSE=0.0094 | val MSE=0.0105\n",
            "Epoch 4/1000 | train MSE=0.0086 | val MSE=0.0046\n",
            "Epoch 5/1000 | train MSE=0.0076 | val MSE=0.0098\n",
            "Epoch 6/1000 | train MSE=0.0071 | val MSE=0.0062\n",
            "Epoch 7/1000 | train MSE=0.0066 | val MSE=0.0077\n",
            "Epoch 8/1000 | train MSE=0.0058 | val MSE=0.0080\n",
            "Epoch 9/1000 | train MSE=0.0054 | val MSE=0.0049\n",
            "Epoch 10/1000 | train MSE=0.0050 | val MSE=0.0052\n",
            "Epoch 11/1000 | train MSE=0.0047 | val MSE=0.0046\n",
            "Epoch 12/1000 | train MSE=0.0043 | val MSE=0.0048\n",
            "Epoch 13/1000 | train MSE=0.0042 | val MSE=0.0046\n",
            "Epoch 14/1000 | train MSE=0.0040 | val MSE=0.0043\n",
            "Epoch 15/1000 | train MSE=0.0038 | val MSE=0.0051\n",
            "Epoch 16/1000 | train MSE=0.0036 | val MSE=0.0051\n",
            "Epoch 17/1000 | train MSE=0.0035 | val MSE=0.0049\n",
            "Epoch 18/1000 | train MSE=0.0034 | val MSE=0.0047\n",
            "Epoch 19/1000 | train MSE=0.0033 | val MSE=0.0055\n",
            "Epoch 20/1000 | train MSE=0.0031 | val MSE=0.0054\n",
            "Epoch 21/1000 | train MSE=0.0030 | val MSE=0.0052\n",
            "Epoch 22/1000 | train MSE=0.0029 | val MSE=0.0046\n",
            "Epoch 23/1000 | train MSE=0.0029 | val MSE=0.0050\n",
            "Epoch 24/1000 | train MSE=0.0027 | val MSE=0.0052\n",
            "Epoch 25/1000 | train MSE=0.0027 | val MSE=0.0046\n",
            "Epoch 26/1000 | train MSE=0.0027 | val MSE=0.0049\n",
            "Epoch 27/1000 | train MSE=0.0025 | val MSE=0.0050\n",
            "Epoch 28/1000 | train MSE=0.0025 | val MSE=0.0060\n",
            "Epoch 29/1000 | train MSE=0.0024 | val MSE=0.0053\n",
            "Epoch 30/1000 | train MSE=0.0022 | val MSE=0.0052\n",
            "Epoch 31/1000 | train MSE=0.0022 | val MSE=0.0054\n",
            "Epoch 32/1000 | train MSE=0.0022 | val MSE=0.0052\n",
            "Epoch 33/1000 | train MSE=0.0021 | val MSE=0.0050\n",
            "Epoch 34/1000 | train MSE=0.0021 | val MSE=0.0049\n",
            "Epoch 35/1000 | train MSE=0.0020 | val MSE=0.0049\n",
            "Epoch 36/1000 | train MSE=0.0020 | val MSE=0.0049\n",
            "Epoch 37/1000 | train MSE=0.0019 | val MSE=0.0048\n",
            "Epoch 38/1000 | train MSE=0.0019 | val MSE=0.0051\n",
            "Epoch 39/1000 | train MSE=0.0018 | val MSE=0.0049\n",
            "Epoch 40/1000 | train MSE=0.0018 | val MSE=0.0050\n",
            "Epoch 41/1000 | train MSE=0.0017 | val MSE=0.0053\n",
            "Epoch 42/1000 | train MSE=0.0017 | val MSE=0.0048\n",
            "Epoch 43/1000 | train MSE=0.0016 | val MSE=0.0051\n",
            "Epoch 44/1000 | train MSE=0.0016 | val MSE=0.0052\n",
            "Epoch 45/1000 | train MSE=0.0016 | val MSE=0.0053\n",
            "Epoch 46/1000 | train MSE=0.0015 | val MSE=0.0048\n",
            "Epoch 47/1000 | train MSE=0.0015 | val MSE=0.0048\n",
            "Epoch 48/1000 | train MSE=0.0015 | val MSE=0.0049\n",
            "Epoch 49/1000 | train MSE=0.0015 | val MSE=0.0049\n",
            "Epoch 50/1000 | train MSE=0.0015 | val MSE=0.0055\n",
            "Epoch 51/1000 | train MSE=0.0014 | val MSE=0.0050\n",
            "Epoch 52/1000 | train MSE=0.0014 | val MSE=0.0053\n",
            "Epoch 53/1000 | train MSE=0.0014 | val MSE=0.0050\n",
            "Epoch 54/1000 | train MSE=0.0013 | val MSE=0.0051\n",
            "Epoch 55/1000 | train MSE=0.0013 | val MSE=0.0050\n",
            "Epoch 56/1000 | train MSE=0.0012 | val MSE=0.0053\n",
            "Epoch 57/1000 | train MSE=0.0013 | val MSE=0.0052\n",
            "Epoch 58/1000 | train MSE=0.0012 | val MSE=0.0053\n",
            "Epoch 59/1000 | train MSE=0.0012 | val MSE=0.0049\n",
            "Epoch 60/1000 | train MSE=0.0012 | val MSE=0.0046\n",
            "Epoch 61/1000 | train MSE=0.0012 | val MSE=0.0047\n",
            "Epoch 62/1000 | train MSE=0.0012 | val MSE=0.0050\n",
            "Epoch 63/1000 | train MSE=0.0012 | val MSE=0.0047\n"
          ]
        }
      ],
      "source": [
        "def run_epoch(loader, train=True, limit_samples=None):\n",
        "    if train:\n",
        "        head.train()\n",
        "    else:\n",
        "        head.eval()\n",
        "    total_loss = 0.0\n",
        "    mini_report_loss = 0.0\n",
        "    sample_num = 0\n",
        "    # batch is only one sample\n",
        "    for batch in loader:\n",
        "        sample_num += 1\n",
        "        if limit_samples is not None and sample_num > limit_samples:\n",
        "            break\n",
        "        # intermediate reports can be run when data set is large and slow\n",
        "        # if sample_num % training_count_report == 0:\n",
        "        #   run_val_report(mini_train_loss=mini_report_loss)\n",
        "        #   mini_report_loss = 0\n",
        "        #   if train: # Ensure head is back in train mode after validation report\n",
        "        #     head.train()\n",
        "\n",
        "        hs, labels = batch[\"encodedFeatures\"].squeeze(0), batch[\"labels\"]\n",
        "        with torch.amp.autocast('cuda', enabled=(device.type==\"cuda\")):\n",
        "            preds = head(hs)\n",
        "            # average of the 4 emotion's square errors\n",
        "            loss = mse(preds, labels)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "        cLoss = loss.item() * labels.size(0)\n",
        "        total_loss += cLoss\n",
        "        mini_report_loss += cLoss\n",
        "\n",
        "    return total_loss / max(1, sample_num)\n",
        "with torch.no_grad():\n",
        "     va_loss = run_epoch(val_loader, train=False)\n",
        "     print(f\"Epoch {0}/{EPOCHS} | val MSE={va_loss:.4f}\")\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss = run_epoch(train_loader, train=True)\n",
        "    if val_loader:\n",
        "        with torch.no_grad():\n",
        "            va_loss = run_epoch(val_loader, train=False)\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f} | val MSE={va_loss:.4f}\")\n",
        "        if va_loss < 0.0041:\n",
        "            break\n",
        "    else:\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsUmON-SwMiY"
      },
      "outputs": [],
      "source": [
        "print(num_mp3s_encoded)\n",
        "print(reused_encodings)\n",
        "print(presaved_encoding_found)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsSrZpMJdWRl"
      },
      "outputs": [],
      "source": [
        "def ccc(y_true, y_pred):\n",
        "    y = np.asarray(y_true, np.float64)\n",
        "    x = np.asarray(y_pred, np.float64)\n",
        "    vx, vy = x.var(), y.var()\n",
        "    mx, my = x.mean(), y.mean()\n",
        "    cov = ((x - mx) * (y - my)).mean()\n",
        "    denom = vx + vy + (mx - my)**2\n",
        "    return float(2 * cov / denom) if denom > 0 else 0.0\n",
        "\n",
        "def evaluate_full(loader):\n",
        "    y_true, y_pred = [], []\n",
        "    head.eval()\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            hs, labels = batch[\"encodedFeatures\"].squeeze(0), batch[\"labels\"]\n",
        "            preds = head(hs)\n",
        "            y_true.append(labels.detach().cpu().numpy())\n",
        "            y_pred.append(preds.detach().cpu().numpy())\n",
        "            torch.cuda.empty_cache()\n",
        "    y_true = np.array(y_true).squeeze(1)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    true_sq = np.array(y_true).squeeze()\n",
        "    pred_sq = np.array(y_pred).squeeze()\n",
        "\n",
        "    mean_true = true_sq.mean(axis=0)\n",
        "    mean_pred = pred_sq.mean(axis=0)\n",
        "    std_true = true_sq.std(axis=0)\n",
        "    std_pred = pred_sq.std(axis=0)\n",
        "    print(\"Keys:            \", TARGET_KEYS)\n",
        "    print(\"Validation mean: \", mean_true)\n",
        "    print(\"Predicted mean:  \", mean_pred)\n",
        "    print(\"Validation std: \", std_true)\n",
        "    print(\"Predicted std:  \", std_pred)\n",
        "\n",
        "    Y = np.concatenate(y_true, axis=0)\n",
        "    P = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    mae = mean_absolute_error(Y, P, multioutput=\"raw_values\")\n",
        "    mse = mean_squared_error(Y, P, multioutput=\"raw_values\")\n",
        "    metrics = {\n",
        "        \"MAE_macro\": float(mae.mean()),\n",
        "        \"MSE_macro\": float(mse.mean()),\n",
        "    }\n",
        "    for i,k in enumerate(TARGET_KEYS):\n",
        "        metrics[f\"MAE_{k}\"] = float(mae[i])\n",
        "        metrics[f\"MSE_{k}\"] = float(mse[i])\n",
        "        metrics[f\"CCC_{k}\"] = ccc(Y[:,i], P[:,i])\n",
        "    return metrics\n",
        "\n",
        "if val_loader and len(val_ds) > 0:\n",
        "    metrics = evaluate_full(val_loader)\n",
        "    print(\"Validation metrics:\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "else:\n",
        "    print(\"No validation split; skipped metrics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da-sMGLJdbMU"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR = \"/content/drive/MyDrive/w2v2_temporal_head_only_humor\"\n",
        "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save torch head\n",
        "torch.save(head.state_dict(), f\"{SAVE_DIR}/temporal_head.pt\")\n",
        "# Save config to rebuild pipeline later\n",
        "json.dump({\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"target_sr\": TARGET_SR,\n",
        "    \"max_seconds\": MAX_SECONDS,\n",
        "    \"target_keys\": TARGET_KEYS,\n",
        "    \"head\": {\"d_model\": int(encoder.config.hidden_size), \"hidden\": 128, \"out_dim\": len(TARGET_KEYS)}\n",
        "}, open(f\"{SAVE_DIR}/config.json\",\"w\"))\n",
        "\n",
        "print(\"Saved to\", SAVE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMMkk3W-_ztR"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/w2v2_temporal_head_only_humor\"\n",
        "\n",
        "# 1) Load config\n",
        "cfg_path = f\"{SAVE_DIR}/config.json\"\n",
        "state_path = f\"{SAVE_DIR}/temporal_head.pt\"\n",
        "\n",
        "with open(cfg_path, \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "MODEL_NAME  = cfg[\"model_name\"]\n",
        "TARGET_SR   = cfg[\"target_sr\"]\n",
        "MAX_SECONDS = cfg[\"max_seconds\"]\n",
        "TARGET_KEYS = cfg[\"target_keys\"]\n",
        "head_cfg    = cfg[\"head\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SDEfok8_2Rj"
      },
      "outputs": [],
      "source": [
        "# Reload frozen base encoder\n",
        "fe = AutoFeatureExtractor.from_pretrained(MODEL_NAME, sampling_rate=TARGET_SR)\n",
        "enc_cfg = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=False)\n",
        "encoder = Wav2Vec2Model.from_pretrained(MODEL_NAME, config=enc_cfg).to(device)\n",
        "encoder.eval()\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Recreate the TemporalHead with same dimensions from config\n",
        "class TemporalHead(nn.Module):\n",
        "    def __init__(self, d_model=768, hidden=128, out_dim=len(TARGET_KEYS)):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(d_model, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Linear(2*hidden, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.LayerNorm(2*hidden),\n",
        "            nn.Linear(2*hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, hs):                 # hs: (B, T', d_model)\n",
        "        z, _ = self.gru(hs)                # (B, T', 2H)\n",
        "        a = self.att(z).squeeze(-1)        # (B, T')\n",
        "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
        "        pooled = (w * z).sum(dim=1)        # (B, 2H)\n",
        "        return self.out(pooled)            # (B, out_dim)\n",
        "\n",
        "\n",
        "head = TemporalHead(\n",
        "    d_model=head_cfg[\"d_model\"],\n",
        "    hidden=head_cfg[\"hidden\"],\n",
        "    out_dim=head_cfg[\"out_dim\"],\n",
        ").to(device)\n",
        "\n",
        "# Load trained weights\n",
        "state = torch.load(state_path, map_location=device)\n",
        "head.load_state_dict(state)\n",
        "head.eval()\n",
        "\n",
        "print(\"Model reloaded and ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-WXvzQS_4xj"
      },
      "outputs": [],
      "source": [
        "def predict_attributes(audio_path: str):\n",
        "    # 1) Load & preprocess audio\n",
        "    wav = load_first_n_seconds(audio_path, TARGET_SR, MAX_SECONDS)  # (T,)\n",
        "    wav = wav.to(device)\n",
        "\n",
        "    # 2) Feature extraction\n",
        "    with torch.no_grad():\n",
        "        inputs = fe(wav, sampling_rate=TARGET_SR, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # 3) Encoder\n",
        "        hs = encoder(**inputs).last_hidden_state  # (1, T', d_model)\n",
        "\n",
        "        # 4) Temporal head\n",
        "        preds = head(hs)  # (1, num_targets)\n",
        "        preds = preds.squeeze(0).cpu().numpy().tolist()\n",
        "    return dict(zip(TARGET_KEYS, preds))\n",
        "\n",
        "# test some examples, including my own voice:\n",
        "result = predict_attributes(\"/content/drive/MyDrive/meLaugh.mp3\")\n",
        "print(result)\n",
        "\n",
        "result = predict_attributes(\"/content/drive/MyDrive/Colab_Drive_Files/worker_3_nov/EN_De1KhiB0-Yo_W000039.mp3\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pYzoOFc19yF"
      },
      "outputs": [],
      "source": [
        "# prints the actual emotion in its expected range\n",
        "print(denormalize_range(result[\"Valence_best\"],\"Valence_best\"))\n",
        "print(denormalize_range(result[\"Arousal_best\"],'Arousal_best'))\n",
        "print(denormalize_range( result[\"Submissive_vs._Dominant_best\"],'Submissive_vs._Dominant_best'))\n",
        "print(denormalize_range(result[\"Serious_vs._Humorous_best\"],'Serious_vs._Humorous_best'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b4d7a9913f8141ba83aa1e1d8d0888cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6318342a707c495ca5ec32ff1630c55a",
              "IPY_MODEL_0a16facb1f79470dbbaa46e680fde028",
              "IPY_MODEL_d8f6f12d7dd64cd8afc46c070fb610e8"
            ],
            "layout": "IPY_MODEL_2bbe872928fd4e10b3a26cfa952b35a9"
          }
        },
        "6318342a707c495ca5ec32ff1630c55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bf6aebe099147aeb9b534e3942af3c5",
            "placeholder": "​",
            "style": "IPY_MODEL_da877c550ec34b51a3977db19bc9f71b",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "0a16facb1f79470dbbaa46e680fde028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6d4c26e875e407988353f8f90fedba8",
            "max": 159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4aca9e0179524f159a67bc49aabe0496",
            "value": 159
          }
        },
        "d8f6f12d7dd64cd8afc46c070fb610e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09eac39f2bd9497cbd8e2cf83c17b751",
            "placeholder": "​",
            "style": "IPY_MODEL_9717efeb0e064174b5dfbf9d334ce5c8",
            "value": " 159/159 [00:00&lt;00:00, 16.0kB/s]"
          }
        },
        "2bbe872928fd4e10b3a26cfa952b35a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bf6aebe099147aeb9b534e3942af3c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da877c550ec34b51a3977db19bc9f71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6d4c26e875e407988353f8f90fedba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aca9e0179524f159a67bc49aabe0496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09eac39f2bd9497cbd8e2cf83c17b751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9717efeb0e064174b5dfbf9d334ce5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb94b89a413f48cb8a1195a94b136ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc217e6120044f9ab34b88cb171bf939",
              "IPY_MODEL_bac8160b904e455cba14fb962b1c0662",
              "IPY_MODEL_fe922d9d703b44159cbadfbeb030a66f"
            ],
            "layout": "IPY_MODEL_3ea54c72237241a2ba623874665756c5"
          }
        },
        "fc217e6120044f9ab34b88cb171bf939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e80bebb204e14ea68655cae31a3292f3",
            "placeholder": "​",
            "style": "IPY_MODEL_7cd72bcb20014383b4a2f9250add96d9",
            "value": "config.json: "
          }
        },
        "bac8160b904e455cba14fb962b1c0662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_183e2ce8e719467ba7c0e6068354e048",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79a7d6b2bef0432383277500db498942",
            "value": 1
          }
        },
        "fe922d9d703b44159cbadfbeb030a66f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de30a400feba4ed3bcc7ea5b438e18fd",
            "placeholder": "​",
            "style": "IPY_MODEL_e451464e484140bd9aa9fcb2ea3c4845",
            "value": " 1.60k/? [00:00&lt;00:00, 151kB/s]"
          }
        },
        "3ea54c72237241a2ba623874665756c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e80bebb204e14ea68655cae31a3292f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cd72bcb20014383b4a2f9250add96d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "183e2ce8e719467ba7c0e6068354e048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "79a7d6b2bef0432383277500db498942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de30a400feba4ed3bcc7ea5b438e18fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e451464e484140bd9aa9fcb2ea3c4845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02dc394f373344739697cebed1a9324c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45c25406802943d8aff52f18866f45f1",
              "IPY_MODEL_aa9f105c64f8463180c0c49955a5accd",
              "IPY_MODEL_8427cbfdd1d74fb0a4438764fa3a20ca"
            ],
            "layout": "IPY_MODEL_8dced1de93d24a05a71ee5d728ba3e0c"
          }
        },
        "45c25406802943d8aff52f18866f45f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03f432a392ec414aa55aebf9ba829905",
            "placeholder": "​",
            "style": "IPY_MODEL_8e81c1c47d2a4dc486f81856608b8613",
            "value": "model.safetensors: 100%"
          }
        },
        "aa9f105c64f8463180c0c49955a5accd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ed6d929c17e41a081630f0890b0434d",
            "max": 377607901,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c3c21311c3d437aae74f00ae3c3960b",
            "value": 377607901
          }
        },
        "8427cbfdd1d74fb0a4438764fa3a20ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85dc542a0b0541209589dbd334ae79e4",
            "placeholder": "​",
            "style": "IPY_MODEL_9a9899a35d954cebbc9a3673fafc67c6",
            "value": " 378M/378M [00:03&lt;00:00, 227MB/s]"
          }
        },
        "8dced1de93d24a05a71ee5d728ba3e0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03f432a392ec414aa55aebf9ba829905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e81c1c47d2a4dc486f81856608b8613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ed6d929c17e41a081630f0890b0434d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c3c21311c3d437aae74f00ae3c3960b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85dc542a0b0541209589dbd334ae79e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a9899a35d954cebbc9a3673fafc67c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}