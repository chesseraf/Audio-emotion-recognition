{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torchcodec --index-url \"https://download.pytorch.org/whl/cu126\""
      ],
      "metadata": {
        "id": "jjw-m6TYMcT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e7564a-5946-43f1-91a1-7a2148742f38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "3rRfPgqacwLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ef19f8-fa97-4c43-f1f4-9727da616ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.8.0+cu126 | CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, gc, torch\n",
        "os.environ[\"USE_TF\"] = \"0\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, random, math\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch, torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoFeatureExtractor, AutoConfig, Wav2Vec2Model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ==== USER SETTINGS ====\n",
        "DRIVE_MOUNTED = True\n",
        "DATA_DIR     = \"/content/drive/MyDrive/Colab_Drive_Files/Worker1\"   # folder with .wav or .mp3 + matching .json\n",
        "if not DRIVE_MOUNTED:\n",
        "    DATA_DIR = \"./Data\"\n",
        "TARGET_KEYS  = [\"Valence_best\",\"Arousal_best\",\"Submissive_vs._Dominant_best\", \"Serious_vs._Humorous_best\"]\n",
        "MODEL_NAME   = \"facebook/wav2vec2-base-960h\"  # small & stable; upgrade later if needed\n",
        "TARGET_SR    = 16_000\n",
        "MAX_SECONDS  = 12.0              # keep modest; you can try more later\n",
        "SEED         = 42\n",
        "\n",
        "# Training\n",
        "EPOCHS       = 30                # start small\n",
        "LR           = 1e-3             # higher LR since we train only a tiny head\n",
        "WEIGHT_DECAY = 0.0\n",
        "BATCH_SIZE   = 1                # keep at 1 for stability\n",
        "NUM_WORKERS  = 0                # 0 = no multiprocessing (stable on Colab)\n",
        "VAL_SPLIT    = 0.1              # if N>1, take ~10% for val\n",
        "MAX_FILES    = None             # set an int (e.g., 200) for a smoke test; None = all\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "P1Q_KHsTc5DQ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization based on label ranges of different attributes\n",
        "ALL_KEYS_WITH_RANGE = {\n",
        "  \"Valence_best\": { \"min\": -3, \"max\": 3 },\n",
        "  \"Arousal_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Submissive_vs._Dominant_best\": { \"min\": -3, \"max\": 3 },\n",
        "  \"Age_best\": { \"min\": 0, \"max\": 6 },\n",
        "  \"Gender_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Serious_vs._Humorous_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Vulnerable_vs._Emotionally_Detached_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Confident_vs._Hesitant_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Warm_vs._Cold_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Monotone_vs._Expressive_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"High-Pitched_vs._Low-Pitched_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Soft_vs._Harsh_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Authenticity_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Recording_Quality_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Background_Noise_best\": { \"min\": 0, \"max\": 3 }\n",
        "}\n",
        "\n",
        "def normalize_range(value, attribute):\n",
        "    min_value = ALL_KEYS_WITH_RANGE[attribute][\"min\"]\n",
        "    max_value = ALL_KEYS_WITH_RANGE[attribute][\"max\"]\n",
        "    return (value - min_value) / (max_value - min_value)\n",
        "def denormalize_range(value, attribute):\n",
        "    min_value = ALL_KEYS_WITH_RANGE[attribute][\"min\"]\n",
        "    max_value = ALL_KEYS_WITH_RANGE[attribute][\"max\"]\n",
        "    return value * (max_value - min_value) + min_value"
      ],
      "metadata": {
        "id": "ZyqMiT2UDXD3"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "975c6090",
        "outputId": "007fe460-7943-4380-f4cb-ff91e54a73f8"
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from typing import List\n",
        "\n",
        "# Audio and emotions will be lazy initialized, and also store their output of the base encoder\n",
        "\n",
        "# Assuming normalize_range, load_first_n_seconds, TARGET_KEYS, TARGET_SR, MAX_SECONDS, fe, device are defined in the global scope\n",
        "\n",
        "class LazyAudioData:\n",
        "    def __init__(self, audio_path: str, json_path: str):\n",
        "        self._audio_path = audio_path\n",
        "        self._json_path = json_path\n",
        "        self._emotions = None  # To store cached emotions\n",
        "        self._wav = None       # To store cached waveform\n",
        "        self._encoded_features = None # To store cached encoded features\n",
        "\n",
        "    @property\n",
        "    def emotions(self) -> List[float]:\n",
        "        if self._emotions is None:\n",
        "            # Load and normalize emotions from JSON using global TARGET_KEYS and normalize_range\n",
        "            try:\n",
        "                emo = json.loads(Path(self._json_path).read_text(encoding=\"utf-8\")).get(\"emotion_annotation\", {})\n",
        "                labels = [normalize_range(float(emo[k]), k) for k in TARGET_KEYS]\n",
        "                if not all(np.isfinite(labels)):\n",
        "                    raise ValueError(\"Non-finite labels found after normalization\")\n",
        "                if isinstance(labels, torch.Tensor):\n",
        "                    labels = labels.unsqueeze(0)          # (1, D)\n",
        "                else:\n",
        "                    labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
        "                labels = labels.to(device)\n",
        "                self._emotions = labels\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading emotions from {self._json_path}: {e}\")\n",
        "                self._emotions = [] # Return empty list on error\n",
        "        return self._emotions\n",
        "\n",
        "    @property\n",
        "    def wav(self) -> torch.Tensor:\n",
        "        if self._wav is None:\n",
        "            # Load waveform using the existing global function and parameters\n",
        "            try:\n",
        "                self._wav = load_first_n_seconds(self._audio_path, TARGET_SR, MAX_SECONDS)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading audio from {self._audio_path}: {e}\")\n",
        "                self._wav = torch.empty(0) # Return empty tensor on error\n",
        "        return self._wav\n",
        "\n",
        "    @property\n",
        "    def encoded_features(self):\n",
        "        if self._encoded_features is None:\n",
        "            # Ensure wav is loaded first\n",
        "            if self._wav is None:\n",
        "                _ = self.wav # Trigger wav loading\n",
        "            if self._wav is not None and self._wav.numel() > 0:\n",
        "                # Process wav through feature extractor (global `fe`)\n",
        "                # Feat should be moved to device for consistency with batch_to_inputs\n",
        "                feat = fe(self._wav, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=\"do_not_pad\")\n",
        "\n",
        "                # TODO THESE ARE ENCODE INPUTS, NOT FEATURES\n",
        "                inputs = {k: v.to(device) for k, v in feat.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    out = encoder(input_values=inputs[\"input_values\"])\n",
        "                    self._encoded_features = out.last_hidden_state  # (B, T', d_model)\n",
        "                # dont keep the large wavs\n",
        "                del self._wav\n",
        "                self._wav = None\n",
        "            else:\n",
        "                self._encoded_features = {} # Return empty dict if no wav or error\n",
        "        return self._encoded_features\n",
        "\n",
        "print(\"LazyAudioData class defined.\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LazyAudioData class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_pairs(data_dir: str, target_keys: List[str], limit: Optional[int]=None):\n",
        "    root = Path(data_dir)\n",
        "    files = sorted(root.rglob(\"*.mp3\"))\n",
        "    items = []\n",
        "    for mp3 in files:\n",
        "        j = mp3.with_suffix(\".json\")\n",
        "        if not j.exists():\n",
        "            continue\n",
        "        items.append(LazyAudioData(str(mp3), str(j)))\n",
        "    if not items:\n",
        "        raise RuntimeError(\"No usable (audio,json) pairs found.\")\n",
        "    return items\n",
        "\n",
        "items = collect_pairs(DATA_DIR, TARGET_KEYS, limit=MAX_FILES)\n",
        "random.shuffle(items)\n",
        "\n",
        "# Split\n",
        "n = len(items)\n",
        "if n == 1:\n",
        "    train_items, val_items = items, []\n",
        "else:\n",
        "    n_val = min(max(1, int(n * VAL_SPLIT)), n-1)\n",
        "    val_items, train_items = items[:n_val], items[n_val:]\n",
        "\n",
        "print(f\"pairs total={n}  train={len(train_items)}  val={len(val_items)}\")"
      ],
      "metadata": {
        "id": "q4YjYyNSc-HN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc1d1a7-5a57-4a01-8d89-3abb061a2721"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pairs total=2404  train=2164  val=240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = int(TARGET_SR * MAX_SECONDS)\n",
        "_resamplers: Dict[Tuple[int,int], torchaudio.transforms.Resample] = {}\n",
        "\n",
        "def load_first_n_seconds(path: str, target_sr: int, max_seconds: float) -> torch.Tensor:\n",
        "    # infer original SR without decoding full file\n",
        "    try:\n",
        "        info = torchaudio.info(path)\n",
        "        orig_sr = info.sample_rate\n",
        "    except Exception:\n",
        "        _, orig_sr = torchaudio.load(path, frame_offset=0, num_frames=1024)\n",
        "    frames = int(orig_sr * max_seconds)\n",
        "\n",
        "    # read only that window\n",
        "    wav, sr = torchaudio.load(path, frame_offset=0, num_frames=frames)  # (C, T<=frames)\n",
        "\n",
        "    # mono\n",
        "    if wav.shape[0] > 1:\n",
        "        wav = wav.mean(0, keepdim=True)\n",
        "    # resample minimal window\n",
        "    if sr != target_sr:\n",
        "        key = (sr, target_sr)\n",
        "        if key not in _resamplers:\n",
        "            _resamplers[key] = torchaudio.transforms.Resample(sr, target_sr)\n",
        "        wav = _resamplers[key](wav)\n",
        "    wav = wav.squeeze(0)\n",
        "\n",
        "    # truncate/pad to EXACT MAX_LEN (so we can use padding=\"do_not_pad\")\n",
        "    if wav.numel() > MAX_LEN:\n",
        "        wav = wav[:MAX_LEN]\n",
        "    if wav.numel() < MAX_LEN:\n",
        "        wav = torch.nn.functional.pad(wav, (0, MAX_LEN - wav.numel()))\n",
        "\n",
        "    # peak normalize\n",
        "    wav = wav / (wav.abs().max() + 1e-9)\n",
        "    return wav\n"
      ],
      "metadata": {
        "id": "DXNs3B1_dBmp"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PathDataset(Dataset):\n",
        "    def __init__(self, items: List[Dict[str,Any]]):\n",
        "        self.items = items\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.items[idx]\n",
        "        return {\"encodedFeatures\": ex.encoded_features, \"labels\": ex.emotions}\n",
        "\n",
        "train_ds = PathDataset(train_items)\n",
        "val_ds   = PathDataset(val_items) if len(val_items) else None\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
        "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False) if val_ds else None\n",
        "\n",
        "print(\"Loaders ready.\")\n"
      ],
      "metadata": {
        "id": "MEMDizHKdLx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd449ff9-03a7-41b1-8134-5e1903922518"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extractor & encoder (frozen)\n",
        "fe = AutoFeatureExtractor.from_pretrained(MODEL_NAME, sampling_rate=TARGET_SR)\n",
        "enc_cfg = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=False)\n",
        "encoder = Wav2Vec2Model.from_pretrained(MODEL_NAME, config=enc_cfg).to(device)\n",
        "encoder.eval()\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Tiny temporal head: GRU + attention pooling -> 3 outputs\n",
        "class TemporalHead(nn.Module):\n",
        "    def __init__(self, d_model=768, hidden=128, out_dim=len(TARGET_KEYS)):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(d_model, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Linear(2*hidden, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.LayerNorm(2*hidden),\n",
        "            nn.Linear(2*hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, hs):                 # hs: (B, T', d_model)\n",
        "        z, _ = self.gru(hs)                # (B, T', 2H)\n",
        "        a = self.att(z).squeeze(-1)        # (B, T')\n",
        "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
        "        pooled = (w * z).sum(dim=1)        # (B, 2H)\n",
        "        return self.out(pooled)            # (B, out_dim)\n",
        "\n",
        "num_labels = len(TARGET_KEYS)\n",
        "head = TemporalHead(d_model=encoder.config.hidden_size, hidden=128, out_dim=num_labels).to(device)\n",
        "\n",
        "# Only head is trainable\n",
        "opt = torch.optim.AdamW(head.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "mse = nn.MSELoss()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
        "\n",
        "print(\"Encoder frozen. Trainable head params:\",\n",
        "      sum(p.numel() for p in head.parameters() if p.requires_grad))\n"
      ],
      "metadata": {
        "id": "qWGws16-dM0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae9076b-c3f7-4515-fda8-757034d1f6fc"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder frozen. Trainable head params: 724229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2202145022.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(loader, train=True):\n",
        "    if train:\n",
        "        head.train()\n",
        "    else:\n",
        "        head.eval()\n",
        "    total_loss = 0.0\n",
        "    n_items = 0\n",
        "    # batch is only one sample\n",
        "    for batch in loader:\n",
        "        hs, labels = batch[\"encodedFeatures\"].squeeze(0), batch[\"labels\"]\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
        "            preds = head(hs)\n",
        "            loss = 0\n",
        "            # preds = preds.squeeze()\n",
        "            # labels = labels.squeeze()\n",
        "            # for i in range(4):\n",
        "            #   v1 = preds[i]\n",
        "            #   v2 = labels[i]\n",
        "            #   loss += (v2 - v1) * (v2 - v1)\n",
        "            loss = mse(preds, labels)\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        n_items += labels.size(0)\n",
        "        # free ASAP\n",
        "        # del inputs, labels, hs, preds, loss\n",
        "        torch.cuda.empty_cache()\n",
        "    return total_loss / max(1, n_items)\n",
        "\n",
        "with torch.no_grad():\n",
        "     va_loss = run_epoch(val_loader, train=False)\n",
        "     print(f\"Epoch {0}/{EPOCHS} | val MSE={va_loss:.4f}\")\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss = run_epoch(train_loader, train=True)\n",
        "    if val_loader:\n",
        "        with torch.no_grad():\n",
        "            va_loss = run_epoch(val_loader, train=False)\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f} | val MSE={va_loss:.4f}\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "m4eXYz08dR3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b85b503-62ed-4293-f3ab-59a93f358a62"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2794914998.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([1, 1, 4])) that is different to the input size (torch.Size([1, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/30 | val MSE=0.0610\n",
            "Epoch 1/30 | train MSE=0.0166 | val MSE=0.0075\n",
            "Epoch 2/30 | train MSE=0.0101 | val MSE=0.0087\n",
            "Epoch 3/30 | train MSE=0.0096 | val MSE=0.0118\n",
            "Epoch 4/30 | train MSE=0.0090 | val MSE=0.0045\n",
            "Epoch 5/30 | train MSE=0.0079 | val MSE=0.0112\n",
            "Epoch 6/30 | train MSE=0.0076 | val MSE=0.0111\n",
            "Epoch 7/30 | train MSE=0.0070 | val MSE=0.0093\n",
            "Epoch 8/30 | train MSE=0.0062 | val MSE=0.0110\n",
            "Epoch 9/30 | train MSE=0.0058 | val MSE=0.0049\n",
            "Epoch 10/30 | train MSE=0.0053 | val MSE=0.0054\n",
            "Epoch 11/30 | train MSE=0.0049 | val MSE=0.0043\n",
            "Epoch 12/30 | train MSE=0.0045 | val MSE=0.0051\n",
            "Epoch 13/30 | train MSE=0.0043 | val MSE=0.0044\n",
            "Epoch 14/30 | train MSE=0.0040 | val MSE=0.0042\n",
            "Epoch 15/30 | train MSE=0.0039 | val MSE=0.0046\n",
            "Epoch 16/30 | train MSE=0.0036 | val MSE=0.0053\n",
            "Epoch 17/30 | train MSE=0.0034 | val MSE=0.0044\n",
            "Epoch 18/30 | train MSE=0.0033 | val MSE=0.0047\n",
            "Epoch 19/30 | train MSE=0.0031 | val MSE=0.0047\n",
            "Epoch 20/30 | train MSE=0.0030 | val MSE=0.0052\n",
            "Epoch 21/30 | train MSE=0.0030 | val MSE=0.0048\n",
            "Epoch 22/30 | train MSE=0.0028 | val MSE=0.0047\n",
            "Epoch 23/30 | train MSE=0.0027 | val MSE=0.0046\n",
            "Epoch 24/30 | train MSE=0.0026 | val MSE=0.0051\n",
            "Epoch 25/30 | train MSE=0.0025 | val MSE=0.0048\n",
            "Epoch 26/30 | train MSE=0.0025 | val MSE=0.0051\n",
            "Epoch 27/30 | train MSE=0.0023 | val MSE=0.0052\n",
            "Epoch 28/30 | train MSE=0.0023 | val MSE=0.0054\n",
            "Epoch 29/30 | train MSE=0.0022 | val MSE=0.0049\n",
            "Epoch 30/30 | train MSE=0.0021 | val MSE=0.0049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ccc(y_true, y_pred):\n",
        "    y = np.asarray(y_true, np.float64)\n",
        "    x = np.asarray(y_pred, np.float64)\n",
        "    vx, vy = x.var(), y.var()\n",
        "    mx, my = x.mean(), y.mean()\n",
        "    cov = ((x - mx) * (y - my)).mean()\n",
        "    denom = vx + vy + (mx - my)**2\n",
        "    return float(2 * cov / denom) if denom > 0 else 0.0\n",
        "\n",
        "def evaluate_full(loader):\n",
        "    y_true, y_pred = [], []\n",
        "    head.eval()\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            hs, labels = batch[\"encodedFeatures\"].squeeze(0), batch[\"labels\"]\n",
        "            preds = head(hs)\n",
        "            y_true.append(labels.detach().cpu().numpy())\n",
        "            y_pred.append(preds.detach().cpu().numpy())\n",
        "            torch.cuda.empty_cache()\n",
        "    y_true = np.array(y_true).squeeze(1)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    true_sq = np.array(y_true).squeeze()\n",
        "    pred_sq = np.array(y_pred).squeeze()\n",
        "\n",
        "    mean_true = true_sq.mean(axis=0)\n",
        "    mean_pred = pred_sq.mean(axis=0)\n",
        "    std_true = true_sq.std(axis=0)\n",
        "    std_pred = pred_sq.std(axis=0)\n",
        "    print(\"Keys:            \", TARGET_KEYS)\n",
        "    print(\"Validation mean: \", mean_true)\n",
        "    print(\"Predicted mean:  \", mean_pred)\n",
        "    print(\"Validation std: \", std_true)\n",
        "    print(\"Predicted std:  \", std_pred)\n",
        "\n",
        "    Y = np.concatenate(y_true, axis=0)\n",
        "    P = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    mae = mean_absolute_error(Y, P, multioutput=\"raw_values\")\n",
        "    mse = mean_squared_error(Y, P, multioutput=\"raw_values\")\n",
        "    metrics = {\n",
        "        \"MAE_macro\": float(mae.mean()),\n",
        "        \"MSE_macro\": float(mse.mean()),\n",
        "    }\n",
        "    for i,k in enumerate(TARGET_KEYS):\n",
        "        metrics[f\"MAE_{k}\"] = float(mae[i])\n",
        "        metrics[f\"MSE_{k}\"] = float(mse[i])\n",
        "        metrics[f\"CCC_{k}\"] = ccc(Y[:,i], P[:,i])\n",
        "    return metrics\n",
        "\n",
        "if val_loader and len(val_ds) > 0:\n",
        "    metrics = evaluate_full(val_loader)\n",
        "    print(\"Validation metrics:\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "else:\n",
        "    print(\"No validation split; skipped metrics.\")\n"
      ],
      "metadata": {
        "id": "KsSrZpMJdWRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea75a97-bcab-412b-d970-3ec009617d74"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys:             ['Valence_best', 'Arousal_best', 'Submissive_vs._Dominant_best', 'Serious_vs._Humorous_best']\n",
            "Validation mean:  [0.5237595  0.30115864 0.541888   0.20898075]\n",
            "Predicted mean:   [0.53760976 0.30992964 0.530504   0.22238731]\n",
            "Validation std:  [0.07485374 0.11105295 0.05251176 0.09412552]\n",
            "Predicted std:   [0.05739578 0.09232868 0.03997454 0.070875  ]\n",
            "Validation metrics:\n",
            "  MAE_macro: 0.0509\n",
            "  MSE_macro: 0.0049\n",
            "  MAE_Valence_best: 0.0517\n",
            "  MSE_Valence_best: 0.0057\n",
            "  CCC_Valence_best: 0.3737\n",
            "  MAE_Arousal_best: 0.0587\n",
            "  MSE_Arousal_best: 0.0061\n",
            "  CCC_Arousal_best: 0.7092\n",
            "  MAE_Submissive_vs._Dominant_best: 0.0333\n",
            "  MSE_Submissive_vs._Dominant_best: 0.0019\n",
            "  CCC_Submissive_vs._Dominant_best: 0.5713\n",
            "  MAE_Serious_vs._Humorous_best: 0.0600\n",
            "  MSE_Serious_vs._Humorous_best: 0.0058\n",
            "  CCC_Serious_vs._Humorous_best: 0.5900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTmiRZfIcVwO",
        "outputId": "8ddbaf4d-78dc-4d18-e463-79202699ab77"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, json\n",
        "SAVE_DIR = \"/content/w2v2_temporal_head\"\n",
        "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save torch head\n",
        "torch.save(head.state_dict(), f\"{SAVE_DIR}/temporal_head.pt\")\n",
        "# Save config to rebuild pipeline later\n",
        "json.dump({\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"target_sr\": TARGET_SR,\n",
        "    \"max_seconds\": MAX_SECONDS,\n",
        "    \"target_keys\": TARGET_KEYS,\n",
        "    \"head\": {\"d_model\": int(encoder.config.hidden_size), \"hidden\": 128, \"out_dim\": len(TARGET_KEYS)}\n",
        "}, open(f\"{SAVE_DIR}/config.json\",\"w\"))\n",
        "\n",
        "print(\"Saved to\", SAVE_DIR)\n"
      ],
      "metadata": {
        "id": "Da-sMGLJdbMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2755950-ebc6-4c98-8437-5043c9f1403e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to /content/w2v2_temporal_head\n"
          ]
        }
      ]
    }
  ]
}