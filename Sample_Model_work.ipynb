{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjw-m6TYMcT6",
        "outputId": "34e7564a-5946-43f1-91a1-7a2148742f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install torchcodec --index-url \"https://download.pytorch.org/whl/cu126\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rRfPgqacwLB",
        "outputId": "c6ef19f8-fa97-4c43-f1f4-9727da616ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.9.0+cu126 | CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, gc, torch\n",
        "os.environ[\"USE_TF\"] = \"0\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "P1Q_KHsTc5DQ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json, random, math\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch, torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoFeatureExtractor, AutoConfig, Wav2Vec2Model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ==== USER SETTINGS ====\n",
        "DRIVE_MOUNTED = True\n",
        "DATA_DIR     = \"/content/drive/MyDrive/worker_3\"   # folder with .wav or .mp3 + matching .json\n",
        "if not DRIVE_MOUNTED:\n",
        "    DATA_DIR = \"./Data\"\n",
        "TARGET_KEYS  = [\"Valence_best\",\"Arousal_best\",\"Submissive_vs._Dominant_best\", \"Serious_vs._Humorous_best\"]\n",
        "MODEL_NAME   = \"facebook/wav2vec2-base-960h\"  # small & stable; upgrade later if needed\n",
        "TARGET_SR    = 16_000\n",
        "MAX_SECONDS  = 12.0              # keep modest; you can try more later\n",
        "SEED         = 42\n",
        "\n",
        "# Training\n",
        "EPOCHS       = 30                # start small\n",
        "LR           = 1e-3             # higher LR since we train only a tiny head\n",
        "WEIGHT_DECAY = 0.0\n",
        "BATCH_SIZE   = 1                # keep at 1 for stability\n",
        "NUM_WORKERS  = 0                # 0 = no multiprocessing (stable on Colab)\n",
        "VAL_SPLIT    = 0.1              # if N>1, take ~10% for val\n",
        "MAX_FILES    = None             # set an int (e.g., 200) for a smoke test; None = all\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "ZyqMiT2UDXD3"
      },
      "outputs": [],
      "source": [
        "# Normalization based on label ranges of different attributes\n",
        "ALL_KEYS_WITH_RANGE = {\n",
        "  \"Valence_best\": { \"min\": -3, \"max\": 3 },\n",
        "  \"Arousal_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Submissive_vs._Dominant_best\": { \"min\": -3, \"max\": 3 },\n",
        "  \"Age_best\": { \"min\": 0, \"max\": 6 },\n",
        "  \"Gender_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Serious_vs._Humorous_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Vulnerable_vs._Emotionally_Detached_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Confident_vs._Hesitant_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Warm_vs._Cold_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Monotone_vs._Expressive_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"High-Pitched_vs._Low-Pitched_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Soft_vs._Harsh_best\": { \"min\": -2, \"max\": 2 },\n",
        "  \"Authenticity_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Recording_Quality_best\": { \"min\": 0, \"max\": 4 },\n",
        "  \"Background_Noise_best\": { \"min\": 0, \"max\": 3 }\n",
        "}\n",
        "\n",
        "def normalize_range(value, attribute):\n",
        "    min_value = ALL_KEYS_WITH_RANGE[attribute][\"min\"]\n",
        "    max_value = ALL_KEYS_WITH_RANGE[attribute][\"max\"]\n",
        "    return (value - min_value) / (max_value - min_value)\n",
        "def denormalize_range(value, attribute):\n",
        "    min_value = ALL_KEYS_WITH_RANGE[attribute][\"min\"]\n",
        "    max_value = ALL_KEYS_WITH_RANGE[attribute][\"max\"]\n",
        "    return value * (max_value - min_value) + min_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "975c6090",
        "outputId": "007fe460-7943-4380-f4cb-ff91e54a73f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LazyAudioData class defined.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from typing import List\n",
        "\n",
        "# Audio and emotions will be lazy initialized, and also store their output of the base encoder\n",
        "\n",
        "# Assuming normalize_range, load_first_n_seconds, TARGET_KEYS, TARGET_SR, MAX_SECONDS, fe, device are defined in the global scope\n",
        "\n",
        "class LazyAudioData:\n",
        "    def __init__(self, audio_path: str, json_path: str):\n",
        "        self._audio_path = audio_path\n",
        "        self._json_path = json_path\n",
        "        self._emotions = None  # To store cached emotions\n",
        "        self._wav = None       # To store cached waveform\n",
        "        self._encoded_features = None # To store cached encoded features\n",
        "\n",
        "    @property\n",
        "    def emotions(self) -> List[float]:\n",
        "        if self._emotions is None:\n",
        "            # Load and normalize emotions from JSON using global TARGET_KEYS and normalize_range\n",
        "            try:\n",
        "                emo = json.loads(Path(self._json_path).read_text(encoding=\"utf-8\")).get(\"emotion_annotation\", {})\n",
        "                labels = [normalize_range(float(emo[k]), k) for k in TARGET_KEYS]\n",
        "                if not all(np.isfinite(labels)):\n",
        "                    raise ValueError(\"Non-finite labels found after normalization\")\n",
        "                if isinstance(labels, torch.Tensor):\n",
        "                    labels = labels.unsqueeze(0)          # (1, D)\n",
        "                else:\n",
        "                    labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
        "                labels = labels.to(device)\n",
        "                self._emotions = labels\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading emotions from {self._json_path}: {e}\")\n",
        "                self._emotions = [] # Return empty list on error\n",
        "        return self._emotions\n",
        "\n",
        "    @property\n",
        "    def wav(self) -> torch.Tensor:\n",
        "        if self._wav is None:\n",
        "            # Load waveform using the existing global function and parameters\n",
        "            try:\n",
        "                self._wav = load_first_n_seconds(self._audio_path, TARGET_SR, MAX_SECONDS)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error loading audio from {self._audio_path}: {e}\")\n",
        "                self._wav = torch.empty(0) # Return empty tensor on error\n",
        "        return self._wav\n",
        "\n",
        "    @property\n",
        "    def encoded_features(self):\n",
        "        if self._encoded_features is None:\n",
        "            # Ensure wav is loaded first\n",
        "            if self._wav is None:\n",
        "                _ = self.wav # Trigger wav loading\n",
        "            if self._wav is not None and self._wav.numel() > 0:\n",
        "                # Process wav through feature extractor (global `fe`)\n",
        "                # Feat should be moved to device for consistency with batch_to_inputs\n",
        "                feat = fe(self._wav, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=\"do_not_pad\")\n",
        "\n",
        "                # TODO THESE ARE ENCODE INPUTS, NOT FEATURES\n",
        "                inputs = {k: v.to(device) for k, v in feat.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    out = encoder(input_values=inputs[\"input_values\"])\n",
        "                    self._encoded_features = out.last_hidden_state  # (B, T', d_model)\n",
        "                # dont keep the large wavs\n",
        "                del self._wav\n",
        "                self._wav = None\n",
        "            else:\n",
        "                self._encoded_features = {} # Return empty dict if no wav or error\n",
        "        return self._encoded_features\n",
        "\n",
        "print(\"LazyAudioData class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4YjYyNSc-HN",
        "outputId": "8bc1d1a7-5a57-4a01-8d89-3abb061a2721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pairs total=510  train=459  val=51\n"
          ]
        }
      ],
      "source": [
        "def collect_pairs(data_dir: str, target_keys: List[str], limit: Optional[int]=None):\n",
        "    root = Path(data_dir)\n",
        "    files = sorted(root.rglob(\"*.mp3\"))\n",
        "    items = []\n",
        "    for mp3 in files:\n",
        "        j = mp3.with_suffix(\".json\")\n",
        "        if not j.exists():\n",
        "            continue\n",
        "        items.append(LazyAudioData(str(mp3), str(j)))\n",
        "    if not items:\n",
        "        raise RuntimeError(\"No usable (audio,json) pairs found.\")\n",
        "    return items\n",
        "\n",
        "items = collect_pairs(DATA_DIR, TARGET_KEYS, limit=MAX_FILES)\n",
        "random.shuffle(items)\n",
        "\n",
        "# Split\n",
        "n = len(items)\n",
        "if n == 1:\n",
        "    train_items, val_items = items, []\n",
        "else:\n",
        "    n_val = min(max(1, int(n * VAL_SPLIT)), n-1)\n",
        "    val_items, train_items = items[:n_val], items[n_val:]\n",
        "\n",
        "print(f\"pairs total={n}  train={len(train_items)}  val={len(val_items)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "DXNs3B1_dBmp"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = int(TARGET_SR * MAX_SECONDS)\n",
        "_resamplers: Dict[Tuple[int,int], torchaudio.transforms.Resample] = {}\n",
        "\n",
        "def load_first_n_seconds(path: str, target_sr: int, max_seconds: float) -> torch.Tensor:\n",
        "    # infer original SR without decoding full file\n",
        "    try:\n",
        "        info = torchaudio.info(path)\n",
        "        orig_sr = info.sample_rate\n",
        "    except Exception:\n",
        "        _, orig_sr = torchaudio.load(path, frame_offset=0, num_frames=1024)\n",
        "    frames = int(orig_sr * max_seconds)\n",
        "\n",
        "    # read only that window\n",
        "    wav, sr = torchaudio.load(path, frame_offset=0, num_frames=frames)  # (C, T<=frames)\n",
        "\n",
        "    # mono\n",
        "    if wav.shape[0] > 1:\n",
        "        wav = wav.mean(0, keepdim=True)\n",
        "    # resample minimal window\n",
        "    if sr != target_sr:\n",
        "        key = (sr, target_sr)\n",
        "        if key not in _resamplers:\n",
        "            _resamplers[key] = torchaudio.transforms.Resample(sr, target_sr)\n",
        "        wav = _resamplers[key](wav)\n",
        "    wav = wav.squeeze(0)\n",
        "\n",
        "    # truncate/pad to EXACT MAX_LEN (so we can use padding=\"do_not_pad\")\n",
        "    if wav.numel() > MAX_LEN:\n",
        "        wav = wav[:MAX_LEN]\n",
        "    if wav.numel() < MAX_LEN:\n",
        "        wav = torch.nn.functional.pad(wav, (0, MAX_LEN - wav.numel()))\n",
        "\n",
        "    # peak normalize\n",
        "    wav = wav / (wav.abs().max() + 1e-9)\n",
        "    return wav\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEMDizHKdLx1",
        "outputId": "cd449ff9-03a7-41b1-8134-5e1903922518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaders ready.\n"
          ]
        }
      ],
      "source": [
        "class PathDataset(Dataset):\n",
        "    def __init__(self, items: List[Dict[str,Any]]):\n",
        "        self.items = items\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.items[idx]\n",
        "        return {\"encodedFeatures\": ex.encoded_features, \"labels\": ex.emotions}\n",
        "\n",
        "train_ds = PathDataset(train_items)\n",
        "val_ds   = PathDataset(val_items) if len(val_items) else None\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
        "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False) if val_ds else None\n",
        "\n",
        "print(\"Loaders ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWGws16-dM0O",
        "outputId": "aae9076b-c3f7-4515-fda8-757034d1f6fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder frozen. Trainable head params: 724229\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2202145022.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n"
          ]
        }
      ],
      "source": [
        "# Feature extractor & encoder (frozen)\n",
        "fe = AutoFeatureExtractor.from_pretrained(MODEL_NAME, sampling_rate=TARGET_SR)\n",
        "enc_cfg = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=False)\n",
        "encoder = Wav2Vec2Model.from_pretrained(MODEL_NAME, config=enc_cfg).to(device)\n",
        "encoder.eval()\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Tiny temporal head: GRU + attention pooling -> 3 outputs\n",
        "class TemporalHead(nn.Module):\n",
        "    def __init__(self, d_model=768, hidden=128, out_dim=len(TARGET_KEYS)):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(d_model, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Linear(2*hidden, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.LayerNorm(2*hidden),\n",
        "            nn.Linear(2*hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, hs):                 # hs: (B, T', d_model)\n",
        "        z, _ = self.gru(hs)                # (B, T', 2H)\n",
        "        a = self.att(z).squeeze(-1)        # (B, T')\n",
        "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
        "        pooled = (w * z).sum(dim=1)        # (B, 2H)\n",
        "        return self.out(pooled)            # (B, out_dim)\n",
        "\n",
        "num_labels = len(TARGET_KEYS)\n",
        "head = TemporalHead(d_model=encoder.config.hidden_size, hidden=128, out_dim=num_labels).to(device)\n",
        "\n",
        "# Only head is trainable\n",
        "opt = torch.optim.AdamW(head.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "mse = nn.MSELoss()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
        "\n",
        "print(\"Encoder frozen. Trainable head params:\",\n",
        "      sum(p.numel() for p in head.parameters() if p.requires_grad))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4eXYz08dR3g",
        "outputId": "1b85b503-62ed-4293-f3ab-59a93f358a62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2794914998.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:634: UserWarning: Using a target size (torch.Size([1, 1, 4])) that is different to the input size (torch.Size([1, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/30 | val MSE=0.0621\n",
            "Epoch 1/30 | train MSE=0.0351 | val MSE=0.0179\n",
            "Epoch 2/30 | train MSE=0.0121 | val MSE=0.0125\n",
            "Epoch 3/30 | train MSE=0.0106 | val MSE=0.0123\n",
            "Epoch 4/30 | train MSE=0.0099 | val MSE=0.0074\n",
            "Epoch 5/30 | train MSE=0.0089 | val MSE=0.0110\n",
            "Epoch 6/30 | train MSE=0.0088 | val MSE=0.0049\n",
            "Epoch 7/30 | train MSE=0.0082 | val MSE=0.0075\n",
            "Epoch 8/30 | train MSE=0.0084 | val MSE=0.0091\n",
            "Epoch 9/30 | train MSE=0.0079 | val MSE=0.0054\n",
            "Epoch 10/30 | train MSE=0.0082 | val MSE=0.0068\n",
            "Epoch 11/30 | train MSE=0.0072 | val MSE=0.0084\n",
            "Epoch 12/30 | train MSE=0.0065 | val MSE=0.0066\n",
            "Epoch 13/30 | train MSE=0.0073 | val MSE=0.0107\n",
            "Epoch 14/30 | train MSE=0.0058 | val MSE=0.0071\n",
            "Epoch 15/30 | train MSE=0.0062 | val MSE=0.0110\n",
            "Epoch 16/30 | train MSE=0.0058 | val MSE=0.0101\n",
            "Epoch 17/30 | train MSE=0.0053 | val MSE=0.0074\n",
            "Epoch 18/30 | train MSE=0.0052 | val MSE=0.0084\n",
            "Epoch 19/30 | train MSE=0.0048 | val MSE=0.0083\n",
            "Epoch 20/30 | train MSE=0.0046 | val MSE=0.0083\n",
            "Epoch 21/30 | train MSE=0.0045 | val MSE=0.0064\n",
            "Epoch 22/30 | train MSE=0.0041 | val MSE=0.0089\n",
            "Epoch 23/30 | train MSE=0.0036 | val MSE=0.0089\n",
            "Epoch 24/30 | train MSE=0.0034 | val MSE=0.0064\n",
            "Epoch 25/30 | train MSE=0.0034 | val MSE=0.0068\n",
            "Epoch 26/30 | train MSE=0.0032 | val MSE=0.0083\n",
            "Epoch 27/30 | train MSE=0.0028 | val MSE=0.0064\n",
            "Epoch 28/30 | train MSE=0.0027 | val MSE=0.0063\n",
            "Epoch 29/30 | train MSE=0.0024 | val MSE=0.0067\n",
            "Epoch 30/30 | train MSE=0.0026 | val MSE=0.0095\n"
          ]
        }
      ],
      "source": [
        "def run_epoch(loader, train=True):\n",
        "    if train:\n",
        "        head.train()\n",
        "    else:\n",
        "        head.eval()\n",
        "    total_loss = 0.0\n",
        "    n_items = 0\n",
        "    # batch is only one sample\n",
        "    for batch in loader:\n",
        "        hs, labels = batch[\"encodedFeatures\"].squeeze(0), batch[\"labels\"]\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
        "            preds = head(hs)\n",
        "            loss = 0\n",
        "            # preds = preds.squeeze()\n",
        "            # labels = labels.squeeze()\n",
        "            # for i in range(4):\n",
        "            #   v1 = preds[i]\n",
        "            #   v2 = labels[i]\n",
        "            #   loss += (v2 - v1) * (v2 - v1)\n",
        "            loss = mse(preds, labels)\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        n_items += labels.size(0)\n",
        "        # free ASAP\n",
        "        # del inputs, labels, hs, preds, loss\n",
        "        torch.cuda.empty_cache()\n",
        "    return total_loss / max(1, n_items)\n",
        "\n",
        "with torch.no_grad():\n",
        "     va_loss = run_epoch(val_loader, train=False)\n",
        "     print(f\"Epoch {0}/{EPOCHS} | val MSE={va_loss:.4f}\")\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss = run_epoch(train_loader, train=True)\n",
        "    if val_loader:\n",
        "        with torch.no_grad():\n",
        "            va_loss = run_epoch(val_loader, train=False)\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f} | val MSE={va_loss:.4f}\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsSrZpMJdWRl",
        "outputId": "dea75a97-bcab-412b-d970-3ec009617d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys:             ['Valence_best', 'Arousal_best', 'Submissive_vs._Dominant_best', 'Serious_vs._Humorous_best']\n",
            "Validation mean:  [0.5399985  0.30235142 0.541286   0.25031593]\n",
            "Predicted mean:   [0.519958   0.23081833 0.61330813 0.1898333 ]\n",
            "Validation std:  [0.08746003 0.10189284 0.04941672 0.09167834]\n",
            "Predicted std:   [0.06748796 0.10269936 0.03366555 0.11437256]\n",
            "Validation metrics:\n",
            "  MAE_macro: 0.0800\n",
            "  MSE_macro: 0.0095\n",
            "  MAE_Valence_best: 0.0582\n",
            "  MSE_Valence_best: 0.0059\n",
            "  CCC_Valence_best: 0.5290\n",
            "  MAE_Arousal_best: 0.0902\n",
            "  MSE_Arousal_best: 0.0118\n",
            "  CCC_Arousal_best: 0.5471\n",
            "  MAE_Submissive_vs._Dominant_best: 0.0738\n",
            "  MSE_Submissive_vs._Dominant_best: 0.0068\n",
            "  CCC_Submissive_vs._Dominant_best: 0.2287\n",
            "  MAE_Serious_vs._Humorous_best: 0.0979\n",
            "  MSE_Serious_vs._Humorous_best: 0.0133\n",
            "  CCC_Serious_vs._Humorous_best: 0.4696\n"
          ]
        }
      ],
      "source": [
        "def ccc(y_true, y_pred):\n",
        "    y = np.asarray(y_true, np.float64)\n",
        "    x = np.asarray(y_pred, np.float64)\n",
        "    vx, vy = x.var(), y.var()\n",
        "    mx, my = x.mean(), y.mean()\n",
        "    cov = ((x - mx) * (y - my)).mean()\n",
        "    denom = vx + vy + (mx - my)**2\n",
        "    return float(2 * cov / denom) if denom > 0 else 0.0\n",
        "\n",
        "def evaluate_full(loader):\n",
        "    y_true, y_pred = [], []\n",
        "    head.eval()\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            hs, labels = batch[\"encodedFeatures\"].squeeze(0), batch[\"labels\"]\n",
        "            preds = head(hs)\n",
        "            y_true.append(labels.detach().cpu().numpy())\n",
        "            y_pred.append(preds.detach().cpu().numpy())\n",
        "            torch.cuda.empty_cache()\n",
        "    y_true = np.array(y_true).squeeze(1)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    true_sq = np.array(y_true).squeeze()\n",
        "    pred_sq = np.array(y_pred).squeeze()\n",
        "\n",
        "    mean_true = true_sq.mean(axis=0)\n",
        "    mean_pred = pred_sq.mean(axis=0)\n",
        "    std_true = true_sq.std(axis=0)\n",
        "    std_pred = pred_sq.std(axis=0)\n",
        "    print(\"Keys:            \", TARGET_KEYS)\n",
        "    print(\"Validation mean: \", mean_true)\n",
        "    print(\"Predicted mean:  \", mean_pred)\n",
        "    print(\"Validation std: \", std_true)\n",
        "    print(\"Predicted std:  \", std_pred)\n",
        "\n",
        "    Y = np.concatenate(y_true, axis=0)\n",
        "    P = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    mae = mean_absolute_error(Y, P, multioutput=\"raw_values\")\n",
        "    mse = mean_squared_error(Y, P, multioutput=\"raw_values\")\n",
        "    metrics = {\n",
        "        \"MAE_macro\": float(mae.mean()),\n",
        "        \"MSE_macro\": float(mse.mean()),\n",
        "    }\n",
        "    for i,k in enumerate(TARGET_KEYS):\n",
        "        metrics[f\"MAE_{k}\"] = float(mae[i])\n",
        "        metrics[f\"MSE_{k}\"] = float(mse[i])\n",
        "        metrics[f\"CCC_{k}\"] = ccc(Y[:,i], P[:,i])\n",
        "    return metrics\n",
        "\n",
        "if val_loader and len(val_ds) > 0:\n",
        "    metrics = evaluate_full(val_loader)\n",
        "    print(\"Validation metrics:\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "else:\n",
        "    print(\"No validation split; skipped metrics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTmiRZfIcVwO",
        "outputId": "8ddbaf4d-78dc-4d18-e463-79202699ab77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da-sMGLJdbMU",
        "outputId": "a2755950-ebc6-4c98-8437-5043c9f1403e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to /content/drive/MyDrive/w2v2_temporal_head\n"
          ]
        }
      ],
      "source": [
        "import joblib, json\n",
        "SAVE_DIR = \"/content/drive/MyDrive/w2v2_temporal_head\"\n",
        "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save torch head\n",
        "torch.save(head.state_dict(), f\"{SAVE_DIR}/temporal_head.pt\")\n",
        "# Save config to rebuild pipeline later\n",
        "json.dump({\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"target_sr\": TARGET_SR,\n",
        "    \"max_seconds\": MAX_SECONDS,\n",
        "    \"target_keys\": TARGET_KEYS,\n",
        "    \"head\": {\"d_model\": int(encoder.config.hidden_size), \"hidden\": 128, \"out_dim\": len(TARGET_KEYS)}\n",
        "}, open(f\"{SAVE_DIR}/config.json\",\"w\"))\n",
        "\n",
        "print(\"Saved to\", SAVE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu4VSsbX_yVZ",
        "outputId": "53aea569-ace8-42c1-b9c3-ff9fe1311876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMMkk3W-_ztR"
      },
      "outputs": [],
      "source": [
        "import json, torch\n",
        "from pathlib import Path\n",
        "from transformers import AutoFeatureExtractor, AutoConfig, Wav2Vec2Model\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/w2v2_temporal_head\"\n",
        "\n",
        "# 1) Load config\n",
        "cfg_path = f\"{SAVE_DIR}/config.json\"\n",
        "state_path = f\"{SAVE_DIR}/temporal_head.pt\"\n",
        "\n",
        "with open(cfg_path, \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "MODEL_NAME  = cfg[\"model_name\"]\n",
        "TARGET_SR   = cfg[\"target_sr\"]\n",
        "MAX_SECONDS = cfg[\"max_seconds\"]\n",
        "TARGET_KEYS = cfg[\"target_keys\"]\n",
        "head_cfg    = cfg[\"head\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289,
          "referenced_widgets": [
            "c7bbaaf93cc44299a8e9e00c97ec7240",
            "03725a26f1684ad3b708eb1a4a9ba29e",
            "b528fc7aa83e4f33928bdfc1b9756458",
            "ca9e946e64094f6aa570a0b48d1ddabb",
            "1d757de0475c4883bd1aa6a86f991dfd",
            "a8eed29150d5473e8f5200dc2df92023",
            "26b77505af7d495796e67dc0c1845029",
            "5e8de8d2eb474c94bf676500dd1e5cf6",
            "7322f8261a474349a7201c0834112897",
            "569ea77c715c41c5af0147e27e2d3733",
            "5dc351f43ddc4025939c2f72c473a5fc",
            "dc9767befb474c2480a41696c1c1a4e2",
            "4551f85253c74f89b85c68dca1e5f54c",
            "cdbd288a39c64e96b8f52811f4e114ee",
            "78e7ca81314640c78c34d162b5f44470",
            "1e84fa94df6940f5a1119be536b08b33",
            "c09d2322437a488e85a196f259e88cec",
            "90566dc9894049089b1e37557ef5a14e",
            "870666b2877e4e77b07b2554b65f8d7f",
            "bb20785341d24b1684c0e301ba619ce4",
            "a781f9da7cd4491d901e43138b216f8e",
            "b3e8ca9d24c04984b4329d9bbe6b9cf4",
            "05ed54103b3d42eab27dccb4c7700a25",
            "29dc7840cf314ff99f745d33f64d05b3",
            "93a1df7c3b8d48b4b24edda1e56c84c3",
            "f42635b649b346d29881a8324bc2c4df",
            "112a944a7bcf4e37a442db5f66a1488a",
            "15631b9924564e9599b70bf85d9071d0",
            "2af81ac080e949c9800d28d8ebf68645",
            "376f574235124f5ea4dcfacfeefe07f4",
            "8509ed5059cd489589123c10f21fe8ab",
            "2d70b5764bfe4645a9e650603db98723",
            "c4bc9e85f66f47538d2ce11717d192b6"
          ]
        },
        "id": "6SDEfok8_2Rj",
        "outputId": "c9e7cd3b-cd6a-4ea2-c3bc-bf9fe6a04e13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7bbaaf93cc44299a8e9e00c97ec7240",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc9767befb474c2480a41696c1c1a4e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05ed54103b3d42eab27dccb4c7700a25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model reloaded and ready!\n"
          ]
        }
      ],
      "source": [
        "# Reload frozen base encoder\n",
        "fe = AutoFeatureExtractor.from_pretrained(MODEL_NAME, sampling_rate=TARGET_SR)\n",
        "enc_cfg = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=False)\n",
        "encoder = Wav2Vec2Model.from_pretrained(MODEL_NAME, config=enc_cfg).to(device)\n",
        "encoder.eval()\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Recreate the TemporalHead with same dimensions from config\n",
        "class TemporalHead(nn.Module):\n",
        "    def __init__(self, d_model=768, hidden=128, out_dim=len(TARGET_KEYS)):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(d_model, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Linear(2*hidden, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.LayerNorm(2*hidden),\n",
        "            nn.Linear(2*hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, hs):                 # hs: (B, T', d_model)\n",
        "        z, _ = self.gru(hs)                # (B, T', 2H)\n",
        "        a = self.att(z).squeeze(-1)        # (B, T')\n",
        "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
        "        pooled = (w * z).sum(dim=1)        # (B, 2H)\n",
        "        return self.out(pooled)            # (B, out_dim)\n",
        "\n",
        "\n",
        "head = TemporalHead(\n",
        "    d_model=head_cfg[\"d_model\"],\n",
        "    hidden=head_cfg[\"hidden\"],\n",
        "    out_dim=head_cfg[\"out_dim\"],\n",
        ").to(device)\n",
        "\n",
        "# Load trained weights\n",
        "state = torch.load(state_path, map_location=device)\n",
        "head.load_state_dict(state)\n",
        "head.eval()\n",
        "\n",
        "print(\"Model reloaded and ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-WXvzQS_4xj",
        "outputId": "20ac9476-8c4a-4291-f123-232091131856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Valence_best': 0.6869761347770691, 'Arousal_best': 0.4310108423233032, 'Submissive_vs._Dominant_best': 0.6740517616271973, 'Serious_vs._Humorous_best': 0.4167667031288147}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def predict_attributes(audio_path: str):\n",
        "    # 1) Load & preprocess audio\n",
        "    wav = load_first_n_seconds(audio_path, TARGET_SR, MAX_SECONDS)  # (T,)\n",
        "    wav = wav.to(device)\n",
        "\n",
        "    # 2) Feature extraction\n",
        "    with torch.no_grad():\n",
        "        inputs = fe(wav, sampling_rate=TARGET_SR, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # 3) Encoder\n",
        "        hs = encoder(**inputs).last_hidden_state  # (1, T', d_model)\n",
        "\n",
        "        # 4) Temporal head\n",
        "        preds = head(hs)  # (1, num_targets)\n",
        "        preds = preds.squeeze(0).cpu().numpy().tolist()\n",
        "\n",
        "    # preds are in normalized space (0–1). If you want to denormalize:\n",
        "    # from your normalize_range() you can build an inverse.\n",
        "\n",
        "    return dict(zip(TARGET_KEYS, preds))\n",
        "\n",
        "# Example:\n",
        "result = predict_attributes(\"/content/drive/MyDrive/Untitled Folder/EN_3eBI-m17n3c_W000000.mp3\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pYzoOFc19yF",
        "outputId": "51cec89e-c060-42c8-a1a9-59f5651c46cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.1218568086624146\n",
            "1.724043369293213\n",
            "1.0443105697631836\n",
            "1.6670668125152588\n"
          ]
        }
      ],
      "source": [
        "print(denormalize_range(result[\"Valence_best\"],\"Valence_best\"))\n",
        "print(denormalize_range(result[\"Arousal_best\"],'Arousal_best'))\n",
        "print(denormalize_range( result[\"Submissive_vs._Dominant_best\"],'Submissive_vs._Dominant_best'))\n",
        "print(denormalize_range(result[\"Serious_vs._Humorous_best\"],'Serious_vs._Humorous_best'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
