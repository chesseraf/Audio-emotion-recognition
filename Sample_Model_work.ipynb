{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Clean out TF/Keras (they can grab CUDA memory even if unused)\n",
        "!pip -q uninstall -y tensorflow keras || true\n",
        "# Minimal deps\n",
        "!pip -q install \"transformers>=4.40,<5\" torchaudio soundfile scikit-learn --upgrade"
      ],
      "metadata": {
        "id": "_-1fbYdReNQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3rRfPgqacwLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72db8e6-2fb7-41a6-810a-309744dacf7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.8.0+cu126 | CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, gc, torch\n",
        "os.environ[\"USE_TF\"] = \"0\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, random, math\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch, torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoFeatureExtractor, AutoConfig, Wav2Vec2Model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ==== USER SETTINGS ====\n",
        "DATA_DIR     = \"/content/Data\"   # folder with .wav or .mp3 + matching .json\n",
        "TARGET_KEYS  = [\"Valence_best\",\"Arousal_best\",\"Submissive_vs._Dominant_best\"]\n",
        "\n",
        "MODEL_NAME   = \"facebook/wav2vec2-base-960h\"  # small & stable; upgrade later if needed\n",
        "TARGET_SR    = 16_000\n",
        "MAX_SECONDS  = 8.0              # keep modest; you can try 10 later\n",
        "SEED         = 42\n",
        "\n",
        "# Training\n",
        "EPOCHS       = 3                # start small\n",
        "LR           = 1e-3             # higher LR since we train only a tiny head\n",
        "WEIGHT_DECAY = 0.0\n",
        "BATCH_SIZE   = 1                # keep at 1 for stability\n",
        "NUM_WORKERS  = 0                # 0 = no multiprocessing (stable on Colab)\n",
        "VAL_SPLIT    = 0.1              # if N>1, take ~10% for val\n",
        "MAX_FILES    = None             # set an int (e.g., 200) for a smoke test; None = all\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "P1Q_KHsTc5DQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_pairs(data_dir: str, target_keys: List[str], limit: Optional[int]=None):\n",
        "    root = Path(data_dir)\n",
        "    files = sorted(root.rglob(\"*.wav\")) or sorted(root.rglob(\"*.mp3\"))\n",
        "    items = []\n",
        "    for a in files:\n",
        "        j = a.with_suffix(\".json\")\n",
        "        if not j.exists():\n",
        "            continue\n",
        "        try:\n",
        "            emo = json.loads(j.read_text(encoding=\"utf-8\")).get(\"emotion_annotation\", {})\n",
        "            labels = [float(emo[k]) for k in target_keys]\n",
        "            if not all(np.isfinite(labels)):\n",
        "                continue\n",
        "            items.append({\"audio_path\": str(a), \"labels\": labels})\n",
        "            if limit is not None and len(items) >= limit:\n",
        "                break\n",
        "        except Exception:\n",
        "            continue\n",
        "    if not items:\n",
        "        raise RuntimeError(\"No usable (audio,json) pairs found.\")\n",
        "    return items\n",
        "\n",
        "items = collect_pairs(DATA_DIR, TARGET_KEYS, limit=MAX_FILES)\n",
        "random.shuffle(items)\n",
        "\n",
        "# Split\n",
        "n = len(items)\n",
        "if n == 1:\n",
        "    train_items, val_items = items, []\n",
        "else:\n",
        "    n_val = min(max(1, int(n * VAL_SPLIT)), n-1)\n",
        "    val_items, train_items = items[:n_val], items[n_val:]\n",
        "\n",
        "print(f\"pairs total={n}  train={len(train_items)}  val={len(val_items)}\")\n"
      ],
      "metadata": {
        "id": "q4YjYyNSc-HN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bacaf2b-2022-4927-de84-053ab021540b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pairs total=80  train=72  val=8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = int(TARGET_SR * MAX_SECONDS)\n",
        "_resamplers: Dict[Tuple[int,int], torchaudio.transforms.Resample] = {}\n",
        "\n",
        "def load_first_n_seconds(path: str, target_sr: int, max_seconds: float) -> torch.Tensor:\n",
        "    # infer original SR without decoding full file\n",
        "    try:\n",
        "        info = torchaudio.info(path)\n",
        "        orig_sr = info.sample_rate\n",
        "    except Exception:\n",
        "        _, orig_sr = torchaudio.load(path, frame_offset=0, num_frames=1024)\n",
        "    frames = int(orig_sr * max_seconds)\n",
        "\n",
        "    # read only that window\n",
        "    wav, sr = torchaudio.load(path, frame_offset=0, num_frames=frames)  # (C, T<=frames)\n",
        "\n",
        "    # mono\n",
        "    if wav.shape[0] > 1:\n",
        "        wav = wav.mean(0, keepdim=True)\n",
        "    # resample minimal window\n",
        "    if sr != target_sr:\n",
        "        key = (sr, target_sr)\n",
        "        if key not in _resamplers:\n",
        "            _resamplers[key] = torchaudio.transforms.Resample(sr, target_sr)\n",
        "        wav = _resamplers[key](wav)\n",
        "    wav = wav.squeeze(0)\n",
        "\n",
        "    # truncate/pad to EXACT MAX_LEN (so we can use padding=\"do_not_pad\")\n",
        "    if wav.numel() > MAX_LEN:\n",
        "        wav = wav[:MAX_LEN]\n",
        "    if wav.numel() < MAX_LEN:\n",
        "        wav = torch.nn.functional.pad(wav, (0, MAX_LEN - wav.numel()))\n",
        "\n",
        "    # peak normalize\n",
        "    wav = wav / (wav.abs().max() + 1e-9)\n",
        "    return wav\n"
      ],
      "metadata": {
        "id": "DXNs3B1_dBmp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PathDataset(Dataset):\n",
        "    def __init__(self, items: List[Dict[str,Any]]):\n",
        "        self.items = items\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.items[idx]\n",
        "        return {\"audio_path\": ex[\"audio_path\"], \"labels\": torch.tensor(ex[\"labels\"], dtype=torch.float32)}\n",
        "\n",
        "train_ds = PathDataset(train_items)\n",
        "val_ds   = PathDataset(val_items) if len(val_items) else None\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
        "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False, drop_last=False) if val_ds else None\n",
        "\n",
        "print(\"Loaders ready.\")\n"
      ],
      "metadata": {
        "id": "MEMDizHKdLx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d9413a-a84c-44ce-a329-84fcf2ef9c1d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extractor & encoder (frozen)\n",
        "fe = AutoFeatureExtractor.from_pretrained(MODEL_NAME, sampling_rate=TARGET_SR)\n",
        "enc_cfg = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=False)\n",
        "encoder = Wav2Vec2Model.from_pretrained(MODEL_NAME, config=enc_cfg).to(device)\n",
        "encoder.eval()\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Tiny temporal head: GRU + attention pooling -> 3 outputs\n",
        "class TemporalHead(nn.Module):\n",
        "    def __init__(self, d_model=768, hidden=128, out_dim=3):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(d_model, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Linear(2*hidden, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.LayerNorm(2*hidden),\n",
        "            nn.Linear(2*hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, hs):                 # hs: (B, T', d_model)\n",
        "        z, _ = self.gru(hs)                # (B, T', 2H)\n",
        "        a = self.att(z).squeeze(-1)        # (B, T')\n",
        "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
        "        pooled = (w * z).sum(dim=1)        # (B, 2H)\n",
        "        return self.out(pooled)            # (B, out_dim)\n",
        "\n",
        "num_labels = len(TARGET_KEYS)\n",
        "head = TemporalHead(d_model=encoder.config.hidden_size, hidden=128, out_dim=num_labels).to(device)\n",
        "\n",
        "# Only head is trainable\n",
        "opt = torch.optim.AdamW(head.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "mse = nn.MSELoss()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
        "\n",
        "print(\"Encoder frozen. Trainable head params:\",\n",
        "      sum(p.numel() for p in head.parameters() if p.requires_grad))\n"
      ],
      "metadata": {
        "id": "qWGws16-dM0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c966a8b-e677-461c-923b-97ae61c90716"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder frozen. Trainable head params: 723972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1430705716.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "def batch_to_inputs(batch):\n",
        "    \"\"\"\n",
        "    Accepts either:\n",
        "      - list of dicts (when using a no-op collate_fn), or\n",
        "      - dict with batched fields (PyTorch default collate)\n",
        "    and returns (inputs_dict, labels_tensor) with batch size 1.\n",
        "    \"\"\"\n",
        "    # Extract path + labels for a single-sample batch\n",
        "    if isinstance(batch, list):\n",
        "        ex = batch[0]\n",
        "        path = ex[\"audio_path\"]\n",
        "        labels = ex[\"labels\"]\n",
        "        if isinstance(labels, torch.Tensor):\n",
        "            labels = labels.unsqueeze(0)          # (1, D)\n",
        "        else:\n",
        "            labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(0)\n",
        "    elif isinstance(batch, dict):\n",
        "        # default collate: \"audio_path\" -> list[str], \"labels\" -> tensor/list\n",
        "        path = batch[\"audio_path\"][0]\n",
        "        labels = batch[\"labels\"]\n",
        "        if isinstance(labels, torch.Tensor):\n",
        "            labels = labels[0].unsqueeze(0)       # (1, D)\n",
        "        else:\n",
        "            labels = torch.tensor(labels[0], dtype=torch.float32).unsqueeze(0)\n",
        "    else:\n",
        "        raise TypeError(f\"Unexpected batch type: {type(batch)}\")\n",
        "\n",
        "    # Load only the first MAX_SECONDS from disk, fixed-length & normalized\n",
        "    wav = load_first_n_seconds(path, TARGET_SR, MAX_SECONDS)\n",
        "\n",
        "    # Fixed length already â†’ no dynamic padding\n",
        "    feat = fe(wav, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=\"do_not_pad\")\n",
        "    inputs = {k: v.to(device) for k, v in feat.items()}\n",
        "    labels = labels.to(device)\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "def encode(inputs):\n",
        "    with torch.no_grad():\n",
        "        out = encoder(input_values=inputs[\"input_values\"])\n",
        "        hs  = out.last_hidden_state  # (B, T', d_model)\n",
        "    return hs\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    if train:\n",
        "        head.train()\n",
        "    else:\n",
        "        head.eval()\n",
        "    total_loss = 0.0\n",
        "    n_items = 0\n",
        "    for batch in loader:\n",
        "        inputs, labels = batch_to_inputs(batch)\n",
        "        hs = encode(inputs)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
        "            preds = head(hs)\n",
        "            loss = mse(preds, labels)\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        n_items += labels.size(0)\n",
        "        # free ASAP\n",
        "        del inputs, labels, hs, preds, loss\n",
        "        torch.cuda.empty_cache()\n",
        "    return total_loss / max(1, n_items)\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss = run_epoch(train_loader, train=True)\n",
        "    if val_loader:\n",
        "        with torch.no_grad():\n",
        "            va_loss = run_epoch(val_loader, train=False)\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f} | val MSE={va_loss:.4f}\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | train MSE={tr_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "m4eXYz08dR3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b588e05a-6b2e-463e-a3f7-21cb8c654164"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-315879173.py:7: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  info = torchaudio.info(path)\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:20: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:27: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  return AudioMetaData(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "/tmp/ipython-input-2928795604.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | train MSE=0.1809 | val MSE=0.3153\n",
            "Epoch 2/20 | train MSE=0.1600 | val MSE=0.2728\n",
            "Epoch 3/20 | train MSE=0.1587 | val MSE=0.3201\n",
            "Epoch 4/20 | train MSE=0.1617 | val MSE=0.2984\n",
            "Epoch 5/20 | train MSE=0.1124 | val MSE=0.3070\n",
            "Epoch 6/20 | train MSE=0.0964 | val MSE=0.3073\n",
            "Epoch 7/20 | train MSE=0.0813 | val MSE=0.2352\n",
            "Epoch 8/20 | train MSE=0.0663 | val MSE=0.3383\n",
            "Epoch 9/20 | train MSE=0.0438 | val MSE=0.3369\n",
            "Epoch 10/20 | train MSE=0.0482 | val MSE=0.3097\n",
            "Epoch 11/20 | train MSE=0.0303 | val MSE=0.3211\n",
            "Epoch 12/20 | train MSE=0.0258 | val MSE=0.2734\n",
            "Epoch 13/20 | train MSE=0.0199 | val MSE=0.3329\n",
            "Epoch 14/20 | train MSE=0.0224 | val MSE=0.2699\n",
            "Epoch 15/20 | train MSE=0.0168 | val MSE=0.2761\n",
            "Epoch 16/20 | train MSE=0.0213 | val MSE=0.3073\n",
            "Epoch 17/20 | train MSE=0.0162 | val MSE=0.2499\n",
            "Epoch 18/20 | train MSE=0.0107 | val MSE=0.3072\n",
            "Epoch 19/20 | train MSE=0.0112 | val MSE=0.2754\n",
            "Epoch 20/20 | train MSE=0.0089 | val MSE=0.2888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ccc(y_true, y_pred):\n",
        "    y = np.asarray(y_true, np.float64)\n",
        "    x = np.asarray(y_pred, np.float64)\n",
        "    vx, vy = x.var(), y.var()\n",
        "    mx, my = x.mean(), y.mean()\n",
        "    cov = ((x - mx) * (y - my)).mean()\n",
        "    denom = vx + vy + (mx - my)**2\n",
        "    return float(2 * cov / denom) if denom > 0 else 0.0\n",
        "\n",
        "def evaluate_full(loader):\n",
        "    y_true, y_pred = [], []\n",
        "    head.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            inputs, labels = batch_to_inputs(batch)\n",
        "            hs = encode(inputs)\n",
        "            preds = head(hs)\n",
        "            y_true.append(labels.detach().cpu().numpy())\n",
        "            y_pred.append(preds.detach().cpu().numpy())\n",
        "            del inputs, labels, hs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "    Y = np.concatenate(y_true, axis=0)\n",
        "    P = np.concatenate(y_pred, axis=0)\n",
        "    mae = mean_absolute_error(Y, P, multioutput=\"raw_values\")\n",
        "    mse = mean_squared_error(Y, P, multioutput=\"raw_values\")\n",
        "    metrics = {\n",
        "        \"MAE_macro\": float(mae.mean()),\n",
        "        \"MSE_macro\": float(mse.mean()),\n",
        "    }\n",
        "    for i,k in enumerate(TARGET_KEYS):\n",
        "        metrics[f\"MAE_{k}\"] = float(mae[i])\n",
        "        metrics[f\"MSE_{k}\"] = float(mse[i])\n",
        "        metrics[f\"CCC_{k}\"] = ccc(Y[:,i], P[:,i])\n",
        "    return metrics\n",
        "\n",
        "if val_loader and len(val_ds) > 0:\n",
        "    metrics = evaluate_full(val_loader)\n",
        "    print(\"Validation metrics:\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "else:\n",
        "    print(\"No validation split; skipped metrics.\")\n"
      ],
      "metadata": {
        "id": "KsSrZpMJdWRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba96fcdf-75bb-4b85-f4fc-1e3f10f20e1c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-315879173.py:7: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  info = torchaudio.info(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation metrics:\n",
            "  MAE_macro: 0.4424\n",
            "  MSE_macro: 0.2889\n",
            "  MAE_Valence_best: 0.7148\n",
            "  MSE_Valence_best: 0.5886\n",
            "  CCC_Valence_best: 0.0448\n",
            "  MAE_Arousal_best: 0.4734\n",
            "  MSE_Arousal_best: 0.2439\n",
            "  CCC_Arousal_best: 0.3702\n",
            "  MAE_Submissive_vs._Dominant_best: 0.1390\n",
            "  MSE_Submissive_vs._Dominant_best: 0.0341\n",
            "  CCC_Submissive_vs._Dominant_best: 0.6429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, json\n",
        "SAVE_DIR = \"/content/w2v2_temporal_head\"\n",
        "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save torch head\n",
        "torch.save(head.state_dict(), f\"{SAVE_DIR}/temporal_head.pt\")\n",
        "# Save config to rebuild pipeline later\n",
        "json.dump({\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"target_sr\": TARGET_SR,\n",
        "    \"max_seconds\": MAX_SECONDS,\n",
        "    \"target_keys\": TARGET_KEYS,\n",
        "    \"head\": {\"d_model\": int(encoder.config.hidden_size), \"hidden\": 128, \"out_dim\": len(TARGET_KEYS)}\n",
        "}, open(f\"{SAVE_DIR}/config.json\",\"w\"))\n",
        "\n",
        "print(\"Saved to\", SAVE_DIR)\n"
      ],
      "metadata": {
        "id": "Da-sMGLJdbMU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}